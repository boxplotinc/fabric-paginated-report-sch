{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Fabric Paginated Report Batch Executor\n\nA production-ready framework for executing Microsoft Fabric paginated reports with parameter looping, supporting **four flexible parameter sources**, OneLake storage, and full pipeline integration.\n\n---\n\n## Table of Contents\n\n1. [Features](#features)\n2. [Quick Start](#quick-start)\n3. [Architecture](#architecture)\n4. [Parameter Sources](#parameter-sources)\n5. [Configuration Reference](#configuration-reference)\n6. [Usage Examples](#usage-examples)\n7. [Advanced Features](#advanced-features)\n8. [Troubleshooting](#troubleshooting)\n9. [Best Practices](#best-practices)\n\n---\n\n## Features\n\n### Core Capabilities\n- **Four Flexible Parameter Sources**\n  - **Semantic Model** (Power BI Dataset) - Query data with DAX, RLS enforced\n  - **Lakehouse** (Delta Lake) - Native Spark SQL integration, best performance\n  - **JSON Array** - Simple direct input for testing\n  - **Warehouse** (SQL Server) - T-SQL queries for enterprise scenarios\n\n- **Output Options**\n  - OneLake storage with date-based folder hierarchy (`Files/reports/archive/YYYY/MM/DD/`)\n  - Multiple export formats: PDF, XLSX, DOCX, PPTX, PNG\n  - Flexible filename formatting with template support\n  - Custom folder structure with template placeholders\n\n- **Enterprise-Ready**\n  - Retry logic with exponential backoff (3 attempts: 30s, 60s, 120s)\n  - Automatic token refresh for long-running batches (handles 1-hour expiration)\n  - Continue on failure (processes all parameters even if some fail)\n  - Input validation and SQL injection protection\n  - Comprehensive error handling and logging\n\n- **Pipeline Integration**\n  - Returns JSON with file list for downstream processing\n  - Configurable performance tuning parameters\n  - Detailed execution summary with success/failure counts\n  - Pipeline ForEach items reference: `@json(activity('NotebookActivity').output.status.Output.result.exitValue).files`\n\n---\n\n## Quick Start\n\n### 1. Choose Your Parameter Source\n\n| Source | Best For | Setup Effort |\n|--------|----------|--------------|\n| **Semantic Model** | Report-driven parameters, business users | Low |\n| **Lakehouse** | Large lists (1000+), data engineers | Low |\n| **JSON** | Testing, static lists | Minimal |\n| **Warehouse** | Enterprise, complex SQL, RLS | Medium |\n\n### 2. Set Up Parameter Source\n\n**Option A: Semantic Model** (Recommended for business users)\n```json\n{\n  \"report_partitioning_source\": \"semantic_model\",\n  \"semantic_model_workspace_id\": \"workspace-guid\",\n  \"semantic_model_dataset_id\": \"dataset-guid\",\n  \"semantic_model_dax_query\": \"EVALUATE FILTER(DISTINCT('DimCustomer'[CustomerName]), 'DimCustomer'[IsActive] = TRUE)\"\n}\n```\n\n**Option B: Lakehouse** (Recommended for data engineers)\n```sql\n-- Create table in Lakehouse\nCREATE TABLE parameter_config (\n    Category STRING,\n    ParameterValue STRING,\n    IsActive BOOLEAN,\n    SortOrder INT\n) USING DELTA;\n\n-- Insert data\nINSERT INTO parameter_config VALUES\n    ('MonthlyReportCustomers', 'Acme Corp', true, 1),\n    ('MonthlyReportCustomers', 'TechStart Inc', true, 2);\n```\n```json\n{\n  \"report_partitioning_source\": \"lakehouse\",\n  \"lakehouse_table\": \"parameter_config\",\n  \"lakehouse_category\": \"MonthlyReportCustomers\",\n  \"lakehouse_column\": \"ParameterValue\"\n}\n```\n\n**Option C: JSON** (Recommended for testing)\n```json\n{\n  \"report_partitioning_source\": \"json\",\n  \"report_partitioning_values\": \"[\\\"Acme Corp\\\", \\\"TechStart Inc\\\", \\\"Global Solutions\\\"]\"\n}\n```\n\n**Option D: Warehouse** (Recommended for enterprise)\n```sql\n-- Create table in Warehouse\nCREATE TABLE dbo.ParameterConfig (\n    Category NVARCHAR(100),\n    ParameterValue NVARCHAR(500),\n    IsActive BIT,\n    SortOrder INT\n);\n\n-- Insert data\nINSERT INTO dbo.ParameterConfig VALUES\n    ('MonthlyReportCustomers', 'Acme Corp', 1, 1);\n```\n```json\n{\n  \"report_partitioning_source\": \"warehouse\",\n  \"warehouse_name\": \"EnterpriseWarehouse\",\n  \"warehouse_table\": \"dbo.ParameterConfig\",\n  \"warehouse_column\": \"ParameterValue\",\n  \"warehouse_category\": \"MonthlyReportCustomers\"\n}\n```\n\n### 3. Create Pipeline\n\n1. Go to **Pipelines** in Fabric workspace\n2. Create new pipeline ‚Üí Add **Notebook activity**\n3. Select this notebook\n4. Configure parameters (see [Configuration Reference](#configuration-reference))\n5. Add triggers (Daily, Weekly, Monthly)\n6. Run manually to test\n7. Verify files in OneLake: `Files/reports/archive/YYYY/MM/DD/`\n\n---\n\n## Architecture\n\n### Component Structure\n\n```\nPipeline (Scheduled/Manual Trigger)\n  ‚îÇ\n  ‚îú‚îÄ Parameters: workspace_id, report_id, output_format, static_params, etc.\n  ‚îÇ\n  ‚ñº\nFabric Notebook: Report Batch Executor\n  ‚îÇ\n  ‚îú‚îÄ Cell 1: Parameter Definitions (overridden by pipeline)\n  ‚îú‚îÄ Cell 2: Configuration Building & OOP Implementation\n  ‚îÇ   ‚îÇ\n  ‚îÇ   ‚îú‚îÄ InputValidator: Validate and sanitize inputs (GUID, SQL, filenames)\n  ‚îÇ   ‚îú‚îÄ FilenameFormatter: Template-based filename generation with date formatting\n  ‚îÇ   ‚îú‚îÄ TokenManager: Auto-refresh Power BI tokens (45-min interval)\n  ‚îÇ   ‚îú‚îÄ ParameterLoader: Load values from 4 sources with retry logic\n  ‚îÇ   ‚îú‚îÄ PowerBIAPIClient: REST API interactions (export, poll, download)\n  ‚îÇ   ‚îú‚îÄ OneLakeStorage: File operations with date-based hierarchy\n  ‚îÇ   ‚îî‚îÄ PaginatedReportExecutor: Main orchestrator\n  ‚îÇ\n  ‚îî‚îÄ Execution Flow:\n      1. Load partitioning parameter values from configured source\n      2. FOR EACH parameter value:\n         a. Merge static parameters with current value\n         b. Initiate report export via Power BI REST API\n         c. Poll export status until completion\n         d. Download report file\n         e. Save to OneLake with formatted filename\n         f. Retry up to 3 times on failure\n      3. Generate execution summary\n      4. Return JSON result to pipeline\n```\n\n### Execution Flow Details\n\n1. **Parameter Loading**\n   - Query configured source (Semantic Model, Lakehouse, JSON, or Warehouse)\n   - Deduplicate values while preserving order\n   - Validate non-empty result\n   - Warn if large batch (100+ values)\n\n2. **Report Generation Loop**\n   - Merge static params + current partitioning value\n   - Ensure token validity (auto-refresh if needed)\n   - Call Power BI REST API to initiate export\n   - Poll status every 5 seconds (configurable)\n   - Download binary file when ready\n   - Continue to next value on failure\n\n3. **File Storage**\n   - Generate filename using template formatter\n   - Apply date formatting to placeholders\n   - Sanitize for filesystem compatibility\n   - Save to OneLake with date-based folder structure\n   - Log file path and size\n\n4. **Result Summary**\n   - Count successes and failures\n   - Calculate total size and average duration\n   - List all generated file paths\n   - Return JSON for pipeline consumption\n\n---\n\n## Parameter Sources\n\n### 1. Semantic Model (Recommended for Business Users)\n\nQuery data from Power BI semantic models using DAX.\n\n**Advantages:**\n- Uses trusted data from existing reports\n- Row-Level Security (RLS) automatically enforced\n- Includes business logic and calculations\n- Fast with semantic layer caching\n- Business users can maintain in Power BI\n\n**Example DAX Queries:**\n\n```dax\n-- Get all active customers\nEVALUATE FILTER(\n    DISTINCT('DimCustomer'[CustomerName]),\n    'DimCustomer'[IsActive] = TRUE\n)\n\n-- Get top 10 customers by sales\nEVALUATE TOPN(\n    10,\n    SUMMARIZECOLUMNS(\n        'DimCustomer'[CustomerName],\n        \"TotalSales\", SUM('FactSales'[SalesAmount])\n    ),\n    [TotalSales], DESC\n)\n\n-- Get customers with sales in last 12 months\nEVALUATE CALCULATETABLE(\n    DISTINCT('DimCustomer'[CustomerName]),\n    DATESINPERIOD('DimDate'[Date], TODAY(), -12, MONTH)\n)\n\n-- Multiple parameters (Region √ó Category combinations)\nEVALUATE\nSUMMARIZECOLUMNS(\n    'DimGeography'[Region],\n    'DimProduct'[Category]\n)\n```\n\n**Configuration:**\n```json\n{\n  \"report_partitioning_source\": \"semantic_model\",\n  \"semantic_model_workspace_id\": \"12345678-1234-1234-1234-123456789abc\",\n  \"semantic_model_dataset_id\": \"87654321-4321-4321-4321-210987654321\",\n  \"semantic_model_dax_query\": \"EVALUATE DISTINCT('DimCustomer'[CustomerName])\"\n}\n```\n\n---\n\n### 2. Lakehouse (Recommended for Data Engineers)\n\nQuery Delta tables in Lakehouse using Spark SQL.\n\n**Advantages:**\n- Native Spark SQL (no connection strings needed)\n- Best performance for large lists (1000+ values)\n- Easy maintenance via Lakehouse UI or notebooks\n- ACID compliance and time travel\n- Zero authentication overhead\n- Highest throughput\n\n**Table Schema:**\n```sql\nCREATE TABLE parameter_config (\n    Category STRING,          -- Report category/group\n    ParameterName STRING,     -- Parameter display name\n    ParameterValue STRING,    -- Actual value to pass to report\n    IsActive BOOLEAN,         -- Enable/disable without deleting\n    SortOrder INT,           -- Control execution order\n    ValidFrom TIMESTAMP,     -- Optional: date range filtering\n    ValidTo TIMESTAMP,       -- Optional: date range filtering\n    Notes STRING             -- Optional: documentation\n) USING DELTA;\n\n-- Create sample data\nINSERT INTO parameter_config VALUES\n    ('MonthlyReportCustomers', 'Customer A', 'Acme Corp', true, 1, CURRENT_TIMESTAMP(), NULL, 'Active customer'),\n    ('MonthlyReportCustomers', 'Customer B', 'TechStart Inc', true, 2, CURRENT_TIMESTAMP(), NULL, 'Active customer'),\n    ('QuarterlyRegions', 'North America', 'North America', true, 1, CURRENT_TIMESTAMP(), NULL, NULL);\n```\n\n**Configuration:**\n```json\n{\n  \"report_partitioning_source\": \"lakehouse\",\n  \"lakehouse_table\": \"parameter_config\",\n  \"lakehouse_category\": \"MonthlyReportCustomers\",\n  \"lakehouse_column\": \"ParameterValue\",\n  \"lakehouse_filter\": \"\"  // Optional: additional WHERE clause\n}\n```\n\n**Advanced Filtering:**\n```json\n{\n  \"lakehouse_filter\": \"ValidFrom <= CURRENT_TIMESTAMP() AND (ValidTo IS NULL OR ValidTo >= CURRENT_TIMESTAMP())\"\n}\n```\n\n---\n\n### 3. JSON Array (Recommended for Testing)\n\nProvide parameters directly as JSON array.\n\n**Advantages:**\n- Simplest setup (no infrastructure)\n- Perfect for testing and development\n- Good for static, small lists\n- Version control friendly\n- Immediate changes without database updates\n\n**Configuration:**\n```json\n{\n  \"report_partitioning_source\": \"json\",\n  \"report_partitioning_values\": \"[\\\"Acme Corp\\\", \\\"TechStart Inc\\\", \\\"Global Solutions\\\"]\"\n}\n```\n\n**Use Cases:**\n- Testing and proof of concept\n- One-time or ad-hoc reports\n- Very small lists (< 10 items)\n- Quick validation of report parameters\n\n---\n\n### 4. Warehouse (Recommended for Enterprise)\n\nQuery Warehouse tables using T-SQL.\n\n**Advantages:**\n- Familiar T-SQL syntax\n- Complex SQL logic (joins, CTEs, window functions)\n- Row-Level Security (RLS)\n- Column-Level Security (CLS)\n- Integration with existing SQL Server workflows\n- Stored procedures support\n\n**Table Schema:**\n```sql\nCREATE TABLE dbo.ParameterConfig (\n    Category NVARCHAR(100),\n    ParameterName NVARCHAR(100),\n    ParameterValue NVARCHAR(500),\n    IsActive BIT DEFAULT 1,\n    SortOrder INT,\n    CreatedDate DATETIME DEFAULT GETDATE(),\n    ModifiedDate DATETIME DEFAULT GETDATE()\n);\n\n-- Insert sample data\nINSERT INTO dbo.ParameterConfig (Category, ParameterName, ParameterValue, IsActive, SortOrder)\nVALUES\n    ('MonthlyReportCustomers', 'Customer A', 'Acme Corp', 1, 1),\n    ('MonthlyReportCustomers', 'Customer B', 'TechStart Inc', 1, 2);\n```\n\n**Configuration:**\n```json\n{\n  \"report_partitioning_source\": \"warehouse\",\n  \"warehouse_name\": \"EnterpriseWarehouse\",\n  \"warehouse_table\": \"dbo.ParameterConfig\",\n  \"warehouse_column\": \"ParameterValue\",\n  \"warehouse_category\": \"MonthlyReportCustomers\"\n}\n```\n\n**Row-Level Security Example:**\n```sql\n-- Create RLS function\nCREATE FUNCTION dbo.fn_CustomerSecurityPredicate(@AssignedTo NVARCHAR(100))\nRETURNS TABLE WITH SCHEMABINDING AS\nRETURN SELECT 1 AS Result\nWHERE @AssignedTo = USER_NAME() OR USER_NAME() IN (SELECT UserName FROM dbo.Admins);\n\n-- Apply security policy\nCREATE SECURITY POLICY dbo.CustomerReportingPolicy\nADD FILTER PREDICATE dbo.fn_CustomerSecurityPredicate(AssignedTo)\nON dbo.ParameterConfig WITH (STATE = ON);\n```\n\n---\n\n## Configuration Reference\n\n### Required Parameters\n\n| Parameter | Type | Description | Example |\n|-----------|------|-------------|---------|\n| `workspace_id` | GUID | Fabric workspace GUID | `\"12345678-1234-...\"` |\n| `report_id` | GUID | Paginated report GUID | `\"87654321-4321-...\"` |\n| `output_format` | String | Export format | `\"PDF\"`, `\"XLSX\"`, `\"DOCX\"`, `\"PPTX\"`, `\"PNG\"` |\n| `static_params` | JSON | Fixed parameters for all reports | `\"{\\\"start_date\\\": \\\"2024-01-01\\\", \\\"end_date\\\": \\\"2024-12-31\\\"}\"` |\n| `report_partitioning_column` | String | Parameter name to loop through | `\"Customer\"`, `\"Region\"`, `\"Department\"` |\n| `report_partitioning_source` | String | Parameter source type | `\"semantic_model\"`, `\"lakehouse\"`, `\"json\"`, `\"warehouse\"` |\n\n### Filename Template Parameters (NEW)\n\n| Parameter | Type | Description | Example |\n|-----------|------|-------------|---------|\n| `report_name` | String | Filename template with placeholders | `\"Report_<report_partitioning_column>_<report_partitioning_value>_<timestamp:yyyymmdd_hhmmss>\"` |\n| `lakehouse_folder` | String | Custom folder path template (optional) | `\"reports/<report_partitioning_column>/<timestamp:yyyy>/<timestamp:mm>\"` |\n\n**Template Placeholders:**\n- `<report_partitioning_column>` - Parameter name (e.g., \"Customer\")\n- `<report_partitioning_value>` - Current value (e.g., \"Acme Corp\")\n- `<timestamp>` or `<timestamp:format>` - Current timestamp\n- `<param_name>` - Any parameter from `static_params`\n- `<param_name:format>` - Date parameter with custom format\n\n**Format Patterns:**\n- `yyyy` - 4-digit year (2025)\n- `yy` - 2-digit year (25)\n- `mm` - 2-digit month (01-12)\n- `dd` - 2-digit day (01-31)\n- `hh` - 2-digit hour (00-23)\n- `MM` - 2-digit minute (00-59)\n- `ss` - 2-digit second (00-59)\n- `ww` - ISO week number (01-53)\n\n**Examples:**\n```python\n# Example 1: Simple filename\nreport_name = \"Report_<report_partitioning_value>_<timestamp:yyyymmdd>\"\n# Output: Report_AcmeCorp_20250130.pdf\n\n# Example 2: Include parameter name\nreport_name = \"<report_partitioning_column>_<report_partitioning_value>_<timestamp:yyyy_mm_dd_hhMMss>\"\n# Output: Customer_AcmeCorp_2025_01_30_143022.pdf\n\n# Example 3: Include static parameters (date range)\nreport_name = \"SalesReport_<start_date:yyyymmdd>_to_<end_date:yyyymmdd>_<report_partitioning_value>\"\n# Output: SalesReport_20240101_to_20241231_AcmeCorp.pdf\n\n# Example 4: Custom folder structure\nlakehouse_folder = \"reports/<report_partitioning_column>/<timestamp:yyyy>/<timestamp:mm>\"\n# Output path: Files/reports/Customer/2025/01/Report_AcmeCorp.pdf\n\n# Example 5: Organize by parameter value\nlakehouse_folder = \"archive/<report_partitioning_value>/<timestamp:yyyy_mm>\"\n# Output path: Files/archive/AcmeCorp/2025_01/Report_AcmeCorp.pdf\n```\n\n### Source-Specific Parameters\n\n**Semantic Model:**\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| `semantic_model_workspace_id` | GUID | Workspace containing semantic model |\n| `semantic_model_dataset_id` | GUID | Semantic model (dataset) GUID |\n| `semantic_model_dax_query` | DAX | DAX query to get parameter values |\n\n**Lakehouse:**\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| `lakehouse_table` | String | Delta table name |\n| `lakehouse_category` | String | Category filter value |\n| `lakehouse_column` | String | Column containing parameter values |\n| `lakehouse_filter` | String | Optional: additional WHERE clause |\n\n**JSON:**\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| `report_partitioning_values` | JSON Array | JSON array of parameter values |\n\n**Warehouse:**\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| `warehouse_name` | String | Warehouse name |\n| `warehouse_table` | String | Table name (e.g., \"dbo.ParameterConfig\") |\n| `warehouse_column` | String | Column containing parameter values |\n| `warehouse_category` | String | Category filter value |\n\n### Execution Options\n\n| Parameter | Type | Default | Description |\n|-----------|------|---------|-------------|\n| `archive_to_onelake` | String | `\"true\"` | Save files to OneLake |\n| `max_retries` | String | `\"3\"` | Retry attempts per report |\n| `export_timeout_seconds` | String | `\"600\"` | Max seconds to wait for export (10 min) |\n| `poll_interval_seconds` | String | `\"5\"` | Seconds between status polls |\n| `retry_backoff_base` | String | `\"30\"` | Base seconds for exponential backoff (30, 60, 120...) |\n\n### Performance Tuning Parameters\n\n| Parameter | Type | Default | Range | Description |\n|-----------|------|---------|-------|-------------|\n| `download_chunk_size_mb` | String | `\"1\"` | 1-100 | Download chunk size in MB |\n| `file_size_warning_mb` | String | `\"500\"` | 10-5000 | File size threshold for warnings |\n| `connection_timeout_seconds` | String | `\"30\"` | 5-300 | API connection timeout |\n| `download_timeout_seconds` | String | `\"120\"` | 30-600 | File download timeout |\n| `param_loader_retry_attempts` | String | `\"3\"` | 1-10 | Parameter loading retry count |\n| `param_loader_retry_delay_seconds` | String | `\"5\"` | 1-60 | Delay between parameter loading retries |\n| `token_refresh_interval_minutes` | String | `\"45\"` | 5-55 | Auto token refresh interval (must be < 60) |\n\n**When to Adjust Performance Parameters:**\n\n**Large Files (> 100MB):**\n```json\n{\n  \"download_chunk_size_mb\": \"5\",\n  \"file_size_warning_mb\": \"1000\",\n  \"download_timeout_seconds\": \"300\"\n}\n```\n\n**Slow Network:**\n```json\n{\n  \"connection_timeout_seconds\": \"60\",\n  \"download_timeout_seconds\": \"240\"\n}\n```\n\n**Long-Running Batches (1000+ reports):**\n```json\n{\n  \"token_refresh_interval_minutes\": \"30\"\n}\n```\n\n**Unreliable Data Source:**\n```json\n{\n  \"param_loader_retry_attempts\": \"5\",\n  \"param_loader_retry_delay_seconds\": \"10\"\n}\n```\n\n---\n\n## Usage Examples\n\n### Example 1: Monthly Customer Reports (Semantic Model)\n\n**Scenario:** Generate monthly sales report for each active customer using data from Power BI semantic model.\n\n```json\n{\n  \"workspace_id\": \"12345678-1234-1234-1234-123456789abc\",\n  \"report_id\": \"87654321-4321-4321-4321-210987654321\",\n  \"output_format\": \"PDF\",\n  \"static_params\": \"{\\\"start_date\\\": \\\"2024-01-01\\\", \\\"end_date\\\": \\\"2024-12-31\\\"}\",\n  \"report_partitioning_column\": \"Customer\",\n  \"report_name\": \"MonthlySales_<report_partitioning_value>_<start_date:yyyymm>\",\n  \"lakehouse_folder\": \"\",\n  \n  \"report_partitioning_source\": \"semantic_model\",\n  \"semantic_model_workspace_id\": \"12345678-1234-1234-1234-123456789abc\",\n  \"semantic_model_dataset_id\": \"dataset-guid\",\n  \"semantic_model_dax_query\": \"EVALUATE FILTER(DISTINCT('DimCustomer'[CustomerName]), 'DimCustomer'[IsActive] = TRUE)\",\n  \n  \"archive_to_onelake\": \"true\",\n  \"max_retries\": \"3\"\n}\n```\n\n**Result:** PDF reports saved to `Files/reports/archive/2025/01/30/MonthlySales_AcmeCorp_202401.pdf`\n\n---\n\n### Example 2: Regional Reports (Lakehouse)\n\n**Scenario:** Generate quarterly report for each region using parameter list from Lakehouse.\n\n**Setup Lakehouse:**\n```sql\nINSERT INTO parameter_config (Category, ParameterValue, IsActive, SortOrder)\nVALUES\n    ('QuarterlyRegions', 'North America', true, 1),\n    ('QuarterlyRegions', 'Europe', true, 2),\n    ('QuarterlyRegions', 'Asia Pacific', true, 3),\n    ('QuarterlyRegions', 'Latin America', true, 4);\n```\n\n**Pipeline Configuration:**\n```json\n{\n  \"workspace_id\": \"workspace-guid\",\n  \"report_id\": \"report-guid\",\n  \"output_format\": \"XLSX\",\n  \"static_params\": \"{\\\"quarter\\\": \\\"Q1\\\", \\\"year\\\": \\\"2024\\\"}\",\n  \"report_partitioning_column\": \"Region\",\n  \"report_name\": \"QuarterlyReport_<year>_<quarter>_<report_partitioning_value>\",\n  \"lakehouse_folder\": \"reports/<year>/<quarter>\",\n  \n  \"report_partitioning_source\": \"lakehouse\",\n  \"lakehouse_table\": \"parameter_config\",\n  \"lakehouse_category\": \"QuarterlyRegions\",\n  \"lakehouse_column\": \"ParameterValue\",\n  \"lakehouse_filter\": \"\"\n}\n```\n\n**Result:** Excel reports saved to `Files/reports/2024/Q1/QuarterlyReport_2024_Q1_NorthAmerica.xlsx`\n\n---\n\n### Example 3: Testing with JSON\n\n**Scenario:** Test report generation for 3 specific customers before rolling out to all.\n\n```json\n{\n  \"workspace_id\": \"workspace-guid\",\n  \"report_id\": \"report-guid\",\n  \"output_format\": \"PDF\",\n  \"static_params\": \"{\\\"start_date\\\": \\\"2024-01-01\\\", \\\"end_date\\\": \\\"2024-01-31\\\"}\",\n  \"report_partitioning_column\": \"Customer\",\n  \"report_name\": \"TestReport_<report_partitioning_value>_<timestamp:yyyymmdd>\",\n  \n  \"report_partitioning_source\": \"json\",\n  \"report_partitioning_values\": \"[\\\"Test Customer A\\\", \\\"Test Customer B\\\", \\\"Test Customer C\\\"]\"\n}\n```\n\n**Result:** 3 test reports generated quickly without setting up database tables.\n\n---\n\n### Example 4: Enterprise Warehouse with RLS\n\n**Scenario:** Generate reports for customers visible to current user based on Row-Level Security.\n\n**Warehouse Setup:**\n```sql\n-- Table with RLS\nCREATE TABLE dbo.CustomerReporting (\n    CustomerName NVARCHAR(200),\n    AssignedTo NVARCHAR(100),\n    IsActive BIT\n);\n\n-- RLS policy\nCREATE FUNCTION dbo.fn_CustomerSecurityPredicate(@AssignedTo NVARCHAR(100))\nRETURNS TABLE WITH SCHEMABINDING AS\nRETURN SELECT 1 AS Result\nWHERE @AssignedTo = USER_NAME() OR USER_NAME() IN (SELECT UserName FROM dbo.Admins);\n\nCREATE SECURITY POLICY dbo.CustomerReportingPolicy\nADD FILTER PREDICATE dbo.fn_CustomerSecurityPredicate(AssignedTo)\nON dbo.CustomerReporting WITH (STATE = ON);\n```\n\n**Pipeline Configuration:**\n```json\n{\n  \"report_partitioning_source\": \"warehouse\",\n  \"warehouse_name\": \"EnterpriseWarehouse\",\n  \"warehouse_table\": \"dbo.CustomerReporting\",\n  \"warehouse_column\": \"CustomerName\",\n  \"warehouse_category\": \"\",\n  \"report_name\": \"<report_partitioning_value>_Report_<timestamp:yyyy_mm_dd>\"\n}\n```\n\n**Result:** Each user only generates reports for their assigned customers (RLS enforced automatically).\n\n---\n\n## Advanced Features\n\n### 1. Multiple Parameter Combinations\n\nFor reports requiring multiple parameters, use one of these approaches:\n\n**Approach A: Semantic Model with SUMMARIZECOLUMNS**\n```dax\n-- Returns all combinations of Region √ó Category\nEVALUATE\nSUMMARIZECOLUMNS(\n    'DimGeography'[Region],\n    'DimProduct'[Category]\n)\n```\nResult: One row per combination. Notebook loops through each row.\n\n**Approach B: Pre-compute combinations in Lakehouse**\n```sql\nINSERT INTO parameter_config (Category, ParameterValue, IsActive, SortOrder)\nSELECT\n    'RegionCategoryCombo',\n    CONCAT(Region, '|', Category) AS ParameterValue,\n    true,\n    ROW_NUMBER() OVER (ORDER BY Region, Category)\nFROM region_category_combinations;\n```\nThen parse the combined value in your paginated report.\n\n---\n\n### 2. Conditional Parameter Lists\n\nFilter parameters based on date, status, or other business logic:\n\n**Semantic Model (DAX):**\n```dax\n-- Customers with sales > $10K in last 6 months\nEVALUATE\nFILTER(\n    DISTINCT('DimCustomer'[CustomerName]),\n    'DimCustomer'[IsActive] = TRUE\n    && CALCULATE(\n        SUM('FactSales'[Amount]),\n        DATESINPERIOD('DimDate'[Date], TODAY(), -6, MONTH)\n    ) > 10000\n)\n```\n\n**Lakehouse (SQL):**\n```json\n{\n  \"lakehouse_filter\": \"ValidFrom <= CURRENT_TIMESTAMP() AND (ValidTo IS NULL OR ValidTo >= CURRENT_TIMESTAMP())\"\n}\n```\n\n---\n\n### 3. Dynamic File Naming\n\nThe `report_name` parameter supports template placeholders for flexible naming:\n\n```python\n# Basic template\nreport_name = \"Report_<report_partitioning_value>_<timestamp:yyyymmdd>\"\n# Output: Report_AcmeCorp_20250130.pdf\n\n# With parameter name\nreport_name = \"<report_partitioning_column>_<report_partitioning_value>_<timestamp:yyyy_mm_dd_hhMMss>\"\n# Output: Customer_AcmeCorp_2025_01_30_143022.pdf\n\n# Include static parameters (automatically detects dates)\nstatic_params = \"{\\\"start_date\\\": \\\"2024-01-01\\\", \\\"end_date\\\": \\\"2024-12-31\\\"}\"\nreport_name = \"SalesReport_<start_date:yyyymmdd>_to_<end_date:yyyymmdd>_<report_partitioning_value>\"\n# Output: SalesReport_20240101_to_20241231_AcmeCorp.pdf\n```\n\n**Character Sanitization:**\n- Special characters are removed or replaced with underscores\n- Spaces become underscores\n- Unicode characters are normalized to ASCII\n- Length is enforced (max 255 characters)\n\n---\n\n### 4. Custom Folder Structure\n\nThe `lakehouse_folder` parameter supports template placeholders for organizing files:\n\n```python\n# Organize by year and month\nlakehouse_folder = \"reports/<timestamp:yyyy>/<timestamp:mm>\"\n# Output: Files/reports/2025/01/Report.pdf\n\n# Organize by parameter value\nlakehouse_folder = \"reports/<report_partitioning_column>\"\n# Output: Files/reports/Customer/Report.pdf\n\n# Organize by static parameter and date\nlakehouse_folder = \"archive/<Region>/<timestamp:yyyy>/<timestamp:mm>\"\n# Output: Files/archive/EMEA/2025/01/Report.pdf\n\n# Leave empty for default date-based structure\nlakehouse_folder = \"\"\n# Output: Files/reports/archive/2025/01/30/Report.pdf\n```\n\n---\n\n### 5. Error Handling and Retry\n\n**Retry Logic:**\n- 3 retry attempts per report (configurable via `max_retries`)\n- Exponential backoff delays: 30s, 60s, 120s (configurable via `retry_backoff_base`)\n- Token automatically refreshed before each attempt\n- Continues to next parameter on final failure\n\n**Error Categories:**\n1. **Power BI API Errors:** Authentication, report not found, invalid parameters\n2. **Export Failures:** Timeout, data source unavailable, memory issues\n3. **Storage Errors:** OneLake write failures, permission issues\n4. **Parameter Loading Errors:** Source unavailable, query errors\n\nAll errors are logged with detailed context for troubleshooting.\n\n---\n\n### 6. Token Management\n\n**Automatic Token Refresh:**\n- Power BI tokens expire after 60 minutes\n- TokenManager tracks token age\n- Auto-refreshes every 45 minutes by default (configurable)\n- Prevents mid-batch authentication failures\n- Logged with timestamp for audit trail\n\n**For Long-Running Batches:**\n```json\n{\n  \"token_refresh_interval_minutes\": \"30\"\n}\n```\n\n---\n\n## Troubleshooting\n\n### Common Issues\n\n#### 1. No Parameter Values Loaded\n\n**Symptoms:** \n```\n‚ùå No parameter values loaded from {source}! Check your configuration.\n```\n\n**Solutions by Source:**\n\n**Semantic Model:**\n- Test DAX query in Power BI Desktop first\n- Verify workspace GUID and dataset GUID are correct\n- Check semantic model is published and accessible\n- Ensure query returns at least one column with values\n\n**Lakehouse:**\n- Verify table exists: `SELECT * FROM parameter_config`\n- Check `Category` matches exactly (case-sensitive)\n- Confirm `IsActive = true` rows exist\n- Ensure notebook is attached to correct Lakehouse\n\n**JSON:**\n- Validate JSON syntax at jsonlint.com\n- Ensure array is not empty: `[]` is invalid\n- Check for proper escaping in pipeline parameters\n\n**Warehouse:**\n- Verify Warehouse exists and is accessible\n- Test query in Warehouse query editor\n- Check table name includes schema: `dbo.ParameterConfig`\n- Confirm user has SELECT permission\n\n---\n\n#### 2. Authentication Errors\n\n**Symptoms:**\n```\n‚ùå Failed to refresh Power BI API token\n401 Unauthorized\n```\n\n**Solutions:**\n- Verify managed identity is enabled for workspace\n- Check notebook has permission to:\n  - Report workspace (Contributor or Viewer)\n  - Semantic model (Build permission)\n  - Lakehouse (Read access)\n  - Warehouse (CONNECT and SELECT permissions)\n- Refresh workspace and reload notebook\n- Check if workspace is in trial mode (may have restrictions)\n\n---\n\n#### 3. Report Export Timeout\n\n**Symptoms:**\n```\n‚ùå Export timeout after 600 seconds\n```\n\n**Solutions:**\n- **Simplify Report:** Reduce data volume, remove complex visuals\n- **Increase Timeout:** Set `export_timeout_seconds` to `\"900\"` (15 min)\n- **Check Data Sources:** Ensure underlying data sources are responsive\n- **Validate Parameters:** Test report manually with same parameters in Power BI\n- **Review Report Design:** Complex calculations and large datasets slow exports\n\n---\n\n#### 4. Slow Performance\n\n**Symptoms:**\n- Each report takes > 5 minutes\n- Batch execution exceeds expected time\n\n**Solutions:**\n\n**Optimize Parameter Source:**\n- **Semantic Model:** Use DISTINCT instead of VALUES, add filters\n- **Lakehouse:** Create indexes on `Category` and `IsActive` columns\n- **Warehouse:** Optimize query, add indexes\n\n**Optimize Report:**\n- Reduce data volume (add filters, limit rows)\n- Simplify visuals (remove complex calculations)\n- Pre-aggregate data in data source\n- Use report-level filters\n\n**Parallel Processing:**\nFor very large batches (500+ reports), split across multiple pipelines:\n```json\n// Pipeline 1: A-M customers\n{\"lakehouse_filter\": \"ParameterValue >= 'A' AND ParameterValue < 'N'\"}\n\n// Pipeline 2: N-Z customers\n{\"lakehouse_filter\": \"ParameterValue >= 'N'\"}\n```\n\n---\n\n#### 5. File Not Found in OneLake\n\n**Symptoms:**\n- Execution shows success but file missing in OneLake\n\n**Solutions:**\n- Check `archive_to_onelake` is set to `\"true\"`\n- Verify Lakehouse is attached to notebook\n- Confirm notebook has Write permission to Lakehouse\n- Check OneLake path in execution logs\n- Look in correct folder: `Files/reports/archive/YYYY/MM/DD/`\n- If using custom `lakehouse_folder`, verify the path exists\n\n---\n\n#### 6. Invalid Filename Template\n\n**Symptoms:**\n```\n‚ùå Invalid filename template: {error_message}\n```\n\n**Solutions:**\n- Check for matching angle brackets: `<placeholder>`\n- Use only valid format patterns: `yyyy`, `mm`, `dd`, `hh`, `MM`, `ss`, `ww`\n- Don't use spaces in placeholder names: `<start_date>` not `<start date>`\n- Ensure placeholders reference valid parameters\n- Avoid special characters outside placeholders\n- Keep total filename under 255 characters\n\n---\n\n### Debug Mode\n\nTo run notebook interactively for debugging:\n\n1. Open notebook in Fabric\n2. Set default values in **Cell 1** for all parameters\n3. Run cells one by one to isolate issues\n4. Check output and logs after each cell\n5. Add temporary logging statements if needed:\n   ```python\n   logger.info(f\"DEBUG: variable = {variable}\")\n   ```\n6. Fix issues and re-run\n7. Remove debug statements before production use\n\n---\n\n### Viewing Detailed Logs\n\n**During Execution:**\n- Monitor notebook cell output in real-time\n- Watch for warnings (‚ö†) and errors (‚ùå)\n- Note execution duration per report\n\n**After Execution:**\n- Review pipeline run history\n- Check notebook output in pipeline activity\n- Export execution logs if needed\n- Review OneLake files to verify outputs\n\n**Log Levels:**\n- `INFO` - Normal execution steps\n- `WARNING` - Non-critical issues (large files, retries)\n- `ERROR` - Failed operations (with retry)\n- `CRITICAL` - Unrecoverable failures\n\n---\n\n## Best Practices\n\n### 1. Parameter Source Selection\n\n| Scenario | Recommended Source | Why |\n|----------|-------------------|-----|\n| Business users maintain list | Semantic Model | Power BI interface, familiar tools |\n| Large lists (1000+ values) | Lakehouse | Best performance, scalability |\n| Need Row-Level Security | Semantic Model or Warehouse | RLS enforced automatically |\n| Testing/Development | JSON | Quick setup, no infrastructure |\n| Complex SQL logic needed | Warehouse | Full T-SQL support |\n| Highest performance | Lakehouse | Native Spark, zero auth overhead |\n| Enterprise security requirements | Warehouse | RLS, CLS, audit trails |\n\n---\n\n### 2. File Organization\n\n**Default Structure (Recommended):**\n```\nFiles/reports/archive/\n  /2025/\n    /01/\n      /30/\n        Report_Customer_AcmeCorp_20250130_143022.pdf\n        Report_Customer_TechStart_20250130_143035.pdf\n```\n\n**Custom Structure Examples:**\n```\n# By customer and year-month\nlakehouse_folder = \"reports/<report_partitioning_value>/<timestamp:yyyy_mm>\"\nFiles/reports/AcmeCorp/2025_01/Report.pdf\n\n# By department and quarter\nlakehouse_folder = \"reports/<Department>/<timestamp:yyyy>/Q<timestamp:ww>\"\nFiles/reports/Sales/2025/Q05/Report.pdf\n\n# By region with nested dates\nlakehouse_folder = \"archive/<Region>/<timestamp:yyyy>/<timestamp:mm>\"\nFiles/archive/EMEA/2025/01/Report.pdf\n```\n\n**Cleanup Recommendations:**\n- Archive reports older than 1 year to cheaper storage\n- Implement retention policy (e.g., delete after 2 years)\n- Monitor OneLake capacity regularly\n- Consider compressing old files\n\n---\n\n### 3. Scheduling Best Practices\n\n**Recommended Schedules:**\n- **Daily Reports:** 6:00 AM (off-peak hours)\n- **Weekly Reports:** Monday 7:00 AM\n- **Monthly Reports:** 1st or last day of month, 8:00 AM\n- **Quarterly Reports:** 1st Monday of new quarter\n\n**Avoid:**\n- Running multiple large batches simultaneously (causes throttling)\n- Peak business hours (9 AM - 5 PM) for large batches\n- Running during platform maintenance windows\n\n**Large Batch Strategies:**\n- Split into multiple pipelines running sequentially\n- Use pipeline dependencies to chain executions\n- Stagger start times (Pipeline 1: 6:00 AM, Pipeline 2: 7:00 AM)\n- Monitor execution times and adjust schedules accordingly\n\n---\n\n### 4. Security Best Practices\n\n**Authentication & Authorization:**\n- ‚úÖ Always use managed identity (never hardcode credentials)\n- ‚úÖ Grant minimal required permissions (principle of least privilege)\n- ‚úÖ Use Row-Level Security (RLS) in Semantic Models/Warehouses\n- ‚úÖ Separate service accounts for different environments (Dev/Test/Prod)\n- ‚úÖ Audit parameter access and report generation regularly\n\n**Input Validation:**\n- ‚úÖ All inputs validated by `InputValidator` class\n- ‚úÖ GUIDs validated against format (36 chars, hyphens)\n- ‚úÖ SQL identifiers validated (alphanumeric, underscore only)\n- ‚úÖ SQL injection protection (parameterized queries, string escaping)\n- ‚úÖ Filename sanitization (removes special chars, path traversal)\n\n**Data Protection:**\n- ‚úÖ Sensitive parameters logged at DEBUG level only\n- ‚úÖ Error messages truncated to avoid exposing secrets\n- ‚úÖ Files stored in workspace-specific Lakehouse (isolated)\n- ‚ùå Never commit credentials or GUIDs to version control\n- ‚ùå Never hardcode connection strings or API keys\n\n**Access Control:**\n- Configure Lakehouse permissions carefully\n- Limit who can modify parameter tables\n- Restrict pipeline execution to authorized users\n- Enable audit logging for compliance\n\n---\n\n### 5. Performance Optimization\n\n**Parameter Loading:**\n- **Semantic Model:** Optimize DAX queries, use DISTINCT, add filters\n- **Lakehouse:** Create indexes on `Category`, `IsActive`, `SortOrder`\n- **Warehouse:** Add clustered index on `Category` and `SortOrder`\n- Cache parameter lists if values don't change during execution\n\n**Report Design:**\n- Minimize data volume (filter at source, not in report)\n- Pre-aggregate data in semantic model/warehouse\n- Avoid complex DAX calculations in report visuals\n- Use efficient visual types (tables instead of complex charts)\n- Test report performance with large datasets\n\n**Batch Processing:**\n- For 500+ reports, split into multiple pipelines\n- Use pipeline parameters to partition work:\n  ```json\n  // Pipeline A\n  {\"lakehouse_filter\": \"ParameterValue < 'M'\"}\n  \n  // Pipeline B\n  {\"lakehouse_filter\": \"ParameterValue >= 'M'\"}\n  ```\n- Run pipelines in parallel if infrastructure supports\n- Monitor throttling and adjust concurrency\n\n**Network & Storage:**\n- Increase timeouts for slow connections\n- Use larger chunk sizes for big files\n- Pre-warm connections with a test report\n- Place Lakehouse in same region as workspace\n\n---\n\n### 6. Monitoring & Maintenance\n\n**Regular Tasks:**\n- **Weekly:** Review pipeline success rates and execution times\n- **Monthly:** Check OneLake storage consumption and cleanup old files\n- **Quarterly:** Optimize parameter sources (remove inactive, reorder by frequency)\n- **Annually:** Review and update security permissions\n\n**Metrics to Track:**\n- Success rate (target: > 95%)\n- Average execution time per report\n- Total batch execution time\n- Storage consumption growth rate\n- Error patterns and frequencies\n\n**Alerts to Configure:**\n- Pipeline failure notifications (email/Teams)\n- Execution time exceeds threshold (e.g., > 2 hours)\n- OneLake storage approaching limit\n- Authentication failures\n\n**Documentation:**\n- Maintain parameter category definitions\n- Document report purposes and audiences\n- Keep DAX/SQL queries commented\n- Record configuration changes in version control\n\n---\n\n## Pipeline Integration\n\n### Consuming Notebook Output\n\nThe notebook exits with JSON containing execution results:\n\n```json\n{\n  \"files\": [\n    \"Files/reports/archive/2025/01/30/Report_Customer_A.pdf\",\n    \"Files/reports/archive/2025/01/30/Report_Customer_B.pdf\"\n  ],\n  \"status\": \"success\",  // or \"partial_success\", \"failed\"\n  \"total\": 10,\n  \"success_count\": 10,\n  \"fail_count\": 0,\n  \"total_size_mb\": 45.7,\n  \"avg_duration_seconds\": 23.4,\n  \"total_duration_seconds\": 234.0,\n  \"errors\": [],\n  \"timestamp\": \"2025-01-30T14:30:22Z\",\n  \"parameter_name\": \"Customer\",\n  \"source_type\": \"semantic_model\"\n}\n```\n\n### Using File List in ForEach Activity\n\n**Reference file list in pipeline:**\n```json\n{\n  \"items\": \"@json(activity('NotebookActivity').output.status.Output.result.exitValue).files\"\n}\n```\n\n**Iterate through files:**\n```json\n{\n  \"activities\": [\n    {\n      \"name\": \"ProcessEachFile\",\n      \"type\": \"ForEach\",\n      \"typeProperties\": {\n        \"items\": \"@json(activity('NotebookActivity').output.status.Output.result.exitValue).files\",\n        \"activities\": [\n          {\n            \"name\": \"SendEmail\",\n            \"type\": \"WebActivity\",\n            \"inputs\": {\n              \"url\": \"@{concat('https://api.sendgrid.com/v3/mail/send')}\",\n              \"method\": \"POST\",\n              \"body\": {\n                \"personalizations\": [{\n                  \"to\": [{\"email\": \"@{item().recipient}\"}]\n                }],\n                \"attachments\": [{\n                  \"filename\": \"@{item().filename}\",\n                  \"content\": \"@{base64(activity('ReadFile').output)}\"\n                }]\n              }\n            }\n          }\n        ]\n      }\n    }\n  ]\n}\n```\n\n### Conditional Logic Based on Status\n\n```json\n{\n  \"activities\": [\n    {\n      \"name\": \"NotebookActivity\",\n      \"type\": \"SynapseNotebook\"\n    },\n    {\n      \"name\": \"IfSuccess\",\n      \"type\": \"IfCondition\",\n      \"dependsOn\": [{\"activity\": \"NotebookActivity\", \"dependencyConditions\": [\"Succeeded\"]}],\n      \"typeProperties\": {\n        \"expression\": {\n          \"@equals(json(activity('NotebookActivity').output.status.Output.result.exitValue).status, 'success')\"\n        },\n        \"ifTrueActivities\": [\n          {\"name\": \"SendSuccessEmail\", \"type\": \"WebActivity\"}\n        ],\n        \"ifFalseActivities\": [\n          {\"name\": \"SendFailureAlert\", \"type\": \"WebActivity\"}\n        ]\n      }\n    }\n  ]\n}\n```\n\n---\n\n## Version History\n\n**v1.0** (2025-01-30)\n- Production-ready paginated report batch execution framework\n- Four flexible parameter sources: Semantic Model (DAX), Lakehouse (Spark SQL), JSON, Warehouse (T-SQL)\n- Automatic token refresh for long-running batches (handles 1-hour token expiration)\n- Flexible filename formatting with template placeholders and date formatting\n- Custom folder structure support with template placeholders\n- OneLake archival with date-based folder hierarchy\n- Retry logic with exponential backoff for robust error handling\n- Configurable performance tuning parameters for enterprise scenarios\n- Comprehensive input validation and SQL injection protection\n- Pipeline integration with detailed JSON output for downstream processing\n- Continue-on-failure support for large batch operations\n- Managed identity authentication\n- Object-oriented architecture for maintainability\n\n---\n\n## Additional Resources\n\n**File Structure:**\n```\n/\n‚îú‚îÄ‚îÄ paginated_report_batch_executor.ipynb  # This notebook\n‚îú‚îÄ‚îÄ README.md                               # Detailed documentation\n‚îú‚îÄ‚îÄ config/\n‚îÇ   ‚îú‚îÄ‚îÄ example_semantic_model.json        # Semantic Model example\n‚îÇ   ‚îú‚îÄ‚îÄ example_lakehouse_mode.json        # Lakehouse example\n‚îÇ   ‚îú‚îÄ‚îÄ example_json_mode.json             # JSON example\n‚îÇ   ‚îî‚îÄ‚îÄ example_warehouse_mode.json        # Warehouse example\n‚îî‚îÄ‚îÄ pipeline/\n    ‚îî‚îÄ‚îÄ pipeline_definition.json           # Sample pipeline definition\n```\n\n**Support:**\n- Check README.md for extended documentation\n- Review example configurations in `config/` folder\n- Test with small parameter lists first\n- Contact your Fabric administrator for permissions issues\n\n---\n\n**Author:** Generated with Claude Code  \n**Version:** 1.0  \n**Last Updated:** 2025-01-30\n\n---\n\n**Ready to generate thousands of reports with ease!** üöÄ"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": "# CELL 1: Parameter Definitions\n# These parameters can be overridden by pipeline\n\n# Report configuration\nworkspace_id = \"\"                    # Fabric workspace GUID\nreport_id = \"\"                       # Paginated report GUID\noutput_format = \"XLSX\"                # PDF, XLSX, DOCX, PPTX, PNG\nstatic_params = \"{}\"                 # JSON: {\"start_date\": \"2024-01-01\", \"end_date\": \"2024-12-31\"}\nreport_partitioning_column = \"Producer\"       # Column to partition reports by (loop through values)\nreport_name = \"Report_<report_partitioning_column>_<report_partitioning_value>_<timestamp:yyyymmdd_hhmmss>\"  # Filename template\n\n# Lakehouse folder path (OPTIONAL - uses default if empty)\n# Template supports same placeholders as report_name: <report_partitioning_column>, <report_partitioning_value>, <timestamp:format>, <param_name>\n# Path is relative to Files/ directory (Files/ is automatically prepended)\n# Examples:\n#   \"\"                                              ‚Üí Files/reports/archive/{YYYY}/{MM}/{DD}/ (default)\n#   \"reports/custom\"                                ‚Üí Files/reports/custom/\n#   \"reports/<report_partitioning_column>\"          ‚Üí Files/reports/CustomerName/\n#   \"reports/<timestamp:yyyy>/<timestamp:mm>\"       ‚Üí Files/reports/2025/01/\n#   \"archive/<Region>/<timestamp:yyyy>\"             ‚Üí Files/archive/EMEA/2025/\nlakehouse_folder = \"\"\n\n# ============================================================\n# SOURCE CONFIGURATION (supports 4 sources)\n# ============================================================\nreport_partitioning_source = \"semantic_model\"  # \"semantic_model\" | \"lakehouse\" | \"json\" | \"warehouse\"\n\n# -------------------- OPTION 1: SEMANTIC MODEL (RECOMMENDED) --------------------\nsemantic_model_workspace_id = \"\"       # Workspace containing the semantic model\nsemantic_model_dataset_id = \"\"         # Semantic model (dataset) GUID\nsemantic_model_dax_query = \"\"          # DAX query to get parameter values\n\n# -------------------- OPTION 2: LAKEHOUSE TABLE --------------------\nlakehouse_table = \"parameter_config\"  # Delta table name\nlakehouse_category = \"ProducerList\"   # Category filter\nlakehouse_column = \"ParameterValue\"   # Column containing values\nlakehouse_filter = \"\"                 # Optional: additional WHERE clause\n\n# -------------------- OPTION 3: JSON ARRAY --------------------\nreport_partitioning_values = \"[]\"       # JSON: [\"Producer A\", \"Producer B\", \"Producer C\"]\n\n# -------------------- OPTION 4: WAREHOUSE --------------------\nwarehouse_name = \"\"                   # Warehouse name\nwarehouse_table = \"\"                  # Table name (e.g., \"dbo.ParameterConfig\")\nwarehouse_column = \"\"                 # Column name\nwarehouse_category = \"\"               # Category filter\n\n# ============================================================\n# EXECUTION OPTIONS\n# ============================================================\narchive_to_onelake = \"true\"           # Save to OneLake\nmax_retries = \"3\"                     # Retry attempts per report\nexport_timeout_seconds = \"600\"        # Max seconds to wait for export (10 minutes)\npoll_interval_seconds = \"5\"           # Seconds between status polls\nretry_backoff_base = \"30\"             # Base seconds for exponential backoff (30, 60, 120...)\n\n# ============================================================\n# PERFORMANCE TUNING (OPTIONAL)\n# ============================================================\ndownload_chunk_size_mb = \"1\"          # Download chunk size in MB\nfile_size_warning_mb = \"500\"          # File size warning threshold in MB\nconnection_timeout_seconds = \"30\"     # API connection timeout in seconds\ndownload_timeout_seconds = \"120\"      # Download timeout in seconds\nparam_loader_retry_attempts = \"3\"     # Parameter loading retry attempts\nparam_loader_retry_delay_seconds = \"5\"  # Delay between parameter loading retries\ntoken_refresh_interval_minutes = \"45\"  # Token refresh interval in minutes\n\n# NOTE: Config dictionary is built in Cell 2 AFTER pipeline parameters are injected"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# CELL 2: COMPLETE OOP IMPLEMENTATION\n# ============================================================================\n\n# ============================================================================\n# IMPORTS AND PACKAGE INSTALLATION\n# ============================================================================\n\nimport requests\nimport json\nimport time\nimport re\nimport struct\nimport sys\nimport os\nimport tempfile\nimport unicodedata\nfrom datetime import datetime, timezone, timedelta\nfrom typing import List, Dict, Any, Optional, Tuple\n\ndef install_package(package: str) -> None:\n    \"\"\"Install package if not already installed\"\"\"\n    import subprocess\n    try:\n        __import__(package.replace('-', '_'))\n    except ImportError:\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"-q\"])\n\n# Install semantic-link for semantic model support\ntry:\n    import sempy.fabric as fabric\nexcept ImportError:\n    install_package(\"semantic-link\")\n    import sempy.fabric as fabric\n\n# Fabric imports\nfrom notebookutils import mssparkutils\n\n# ============================================================================\n# LOGGER SETUP\n# ============================================================================\n\nimport logging\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger(__name__)\n\n# ============================================================================\n# BUILD CONFIGURATION DICTIONARY\n# ============================================================================\n# Pipeline parameters are injected between Cell 1 and Cell 2, so we build\n# the config dictionary here to capture the injected values\n\n# Parse JSON parameters\nstatic_params_dict = json.loads(static_params)\n\n# Strip whitespace from GUIDs (in case pipeline adds spaces)\nworkspace_id = workspace_id.strip() if workspace_id else \"\"\nreport_id = report_id.strip() if report_id else \"\"\n\n# Build unified configuration dictionary for executor\nconfig = {\n    # Core report configuration\n    'workspace_id': workspace_id,\n    'report_id': report_id,\n    'output_format': output_format.upper(),\n    'static_params': static_params_dict,\n    'report_partitioning_column': report_partitioning_column,\n    'report_partitioning_source': report_partitioning_source,\n    'report_name': report_name,\n    'lakehouse_folder': lakehouse_folder,  # ADDED: Custom folder path support\n\n    # Source-specific configurations\n    'semantic_model': {\n        'workspace_id': semantic_model_workspace_id.strip() if semantic_model_workspace_id else \"\",\n        'dataset_id': semantic_model_dataset_id.strip() if semantic_model_dataset_id else \"\",\n        'dax_query': semantic_model_dax_query\n    },\n    'lakehouse': {\n        'table': lakehouse_table,\n        'category': lakehouse_category,\n        'column': lakehouse_column,\n        'filter_clause': lakehouse_filter\n    },\n    'json': {\n        'json_values': report_partitioning_values\n    },\n    'warehouse': {\n        'warehouse_name': warehouse_name,\n        'table': warehouse_table,\n        'column': warehouse_column,\n        'category': warehouse_category\n    },\n\n    # Execution settings (convert strings to appropriate types)\n    'archive_to_onelake': archive_to_onelake.lower() == \"true\",\n    'max_retries': int(max_retries),\n    'export_timeout': int(export_timeout_seconds),\n    'poll_interval': int(poll_interval_seconds),\n    'retry_backoff_base': int(retry_backoff_base),\n    'download_chunk_size_mb': int(download_chunk_size_mb),\n    'file_size_warning_mb': int(file_size_warning_mb),\n    'connection_timeout': int(connection_timeout_seconds),\n    'download_timeout': int(download_timeout_seconds),\n    'param_loader_max_retries': int(param_loader_retry_attempts),\n    'param_loader_retry_delay': int(param_loader_retry_delay_seconds),\n    'token_refresh_interval': int(token_refresh_interval_minutes)\n}\n\nlogger.info(\"Config dictionary built successfully\")\nlogger.info(f\"  workspace_id: {config['workspace_id'][:8]}... (length={len(config['workspace_id'])})\")\nlogger.info(f\"  report_id: {config['report_id'][:8]}... (length={len(config['report_id'])})\")\nlogger.info(f\"  report_partitioning_source: {config['report_partitioning_source']}\")\n\n\n# ============================================================================\n# INPUT VALIDATOR CLASS\n# ============================================================================\n\nclass InputValidator:\n    \"\"\"Validate and sanitize user inputs to prevent injection attacks\"\"\"\n\n    @staticmethod\n    def is_valid_guid(value: str) -> bool:\n        \"\"\"Validate GUID format\"\"\"\n        guid_pattern = r'^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$'\n        return bool(re.match(guid_pattern, value.lower())) if value else False\n\n    @staticmethod\n    def is_valid_sql_identifier(value: str) -> bool:\n        \"\"\"Validate SQL identifier (table/column name) - alphanumeric, underscore, dot only\"\"\"\n        pattern = r'^[a-zA-Z_][a-zA-Z0-9_\\.]*$'\n        return bool(re.match(pattern, value)) and len(value) <= 128\n\n    @staticmethod\n    def is_valid_source_type(value: str) -> bool:\n        \"\"\"Validate parameter source type\"\"\"\n        return value in [\"semantic_model\", \"lakehouse\", \"json\", \"warehouse\"]\n\n    @staticmethod\n    def is_valid_format(value: str) -> bool:\n        \"\"\"Validate output format\"\"\"\n        return value.upper() in [\"PDF\", \"XLSX\", \"DOCX\", \"PPTX\", \"PNG\"]\n\n    @staticmethod\n    def sanitize_sql_string(value: str) -> str:\n        \"\"\"Sanitize string for use in SQL - escape single quotes\"\"\"\n        if value is None:\n            return \"\"\n        return str(value).replace(\"'\", \"''\")\n\n    @staticmethod\n    def validate_filename_template(template: str) -> Tuple[bool, Optional[str]]:\n        \"\"\"Validate filename template syntax\n\n        Returns:\n            Tuple of (is_valid, error_message)\n        \"\"\"\n        if not template or len(template) == 0:\n            return False, \"Template cannot be empty\"\n\n        if len(template) > 500:\n            return False, \"Template too long (max 500 characters)\"\n\n        # Check for path traversal attempts\n        if '../' in template or '..\\\\'  in template:\n            return False, \"Template contains path traversal characters\"\n\n        # Parse all placeholders using regex: <placeholder> or <placeholder:format>\n        placeholder_pattern = r'<([^:>]+)(?::([^>]+))?>'\n        placeholders = re.findall(placeholder_pattern, template)\n\n        # Check for unclosed brackets\n        open_count = template.count('<')\n        close_count = template.count('>')\n        if open_count != close_count:\n            return False, f\"Mismatched brackets: {open_count} '<' but {close_count} '>'\"\n\n        # Validate each placeholder\n        valid_format_patterns = ['yyyy', 'yy', 'mm', 'dd', 'ww', 'hh', 'MM', 'ss', '_']\n        for placeholder_name, format_spec in placeholders:\n            if not placeholder_name:\n                return False, \"Empty placeholder name found\"\n\n            # Check placeholder name is valid (alphanumeric and underscore only)\n            if not re.match(r'^[a-zA-Z_][a-zA-Z0-9_]*$', placeholder_name):\n                return False, f\"Invalid placeholder name: '{placeholder_name}' (use only letters, numbers, underscores)\"\n\n            # If format specified, validate it contains only known patterns\n            if format_spec:\n                # Check that format only contains valid date/time patterns\n                temp_format = format_spec\n                for pattern in valid_format_patterns:\n                    temp_format = temp_format.replace(pattern, '')\n                if temp_format:  # If anything remains, it's invalid\n                    return False, f\"Invalid format pattern in '{placeholder_name}:{format_spec}' - unknown characters: '{temp_format}'\"\n\n        return True, None\n\n    @staticmethod\n    def validate_lakehouse_folder_template(template: str) -> Tuple[bool, Optional[str]]:\n        \"\"\"Validate lakehouse folder path template syntax\n\n        Returns:\n            Tuple of (is_valid, error_message)\n        \"\"\"\n        # Empty string is valid (means use default)\n        if not template or len(template.strip()) == 0:\n            return True, None\n\n        if len(template) > 500:\n            return False, \"Folder path template too long (max 500 characters)\"\n\n        # Check for path traversal attempts\n        if '../' in template or '..\\\\'  in template:\n            return False, \"Folder path contains path traversal characters (../)\"\n\n        # Check for trailing slashes\n        if template.endswith('/') or template.endswith('\\\\'):\n            return False, \"Folder path should not end with a slash\"\n\n        # Check for file extensions (common ones)\n        file_extensions = ['.pdf', '.xlsx', '.docx', '.pptx', '.png', '.csv', '.txt', '.json']\n        for ext in file_extensions:\n            if template.lower().endswith(ext):\n                return False, f\"Folder path should not contain file extensions like '{ext}'\"\n\n        # Parse all placeholders using regex: <placeholder> or <placeholder:format>\n        placeholder_pattern = r'<([^:>]+)(?::([^>]+))?>'\n\n        # Check for unclosed brackets\n        open_count = template.count('<')\n        close_count = template.count('>')\n        if open_count != close_count:\n            return False, f\"Mismatched brackets: {open_count} '<' but {close_count} '>'\"\n\n        placeholders = re.findall(placeholder_pattern, template)\n\n        # Validate each placeholder\n        valid_format_patterns = ['yyyy', 'yy', 'mm', 'dd', 'ww', 'hh', 'MM', 'ss', '_', '/']  # Note: '/' added for folder paths\n        for placeholder_name, format_spec in placeholders:\n            if not placeholder_name:\n                return False, \"Empty placeholder name found\"\n\n            # Check placeholder name is valid (alphanumeric and underscore only)\n            if not re.match(r'^[a-zA-Z_][a-zA-Z0-9_]*$', placeholder_name):\n                return False, f\"Invalid placeholder name: '{placeholder_name}' (use only letters, numbers, underscores)\"\n\n            # If format specified, validate it contains only known patterns\n            if format_spec:\n                # Check that format only contains valid date/time patterns and forward slashes\n                temp_format = format_spec\n                for pattern in valid_format_patterns:\n                    temp_format = temp_format.replace(pattern, '')\n                if temp_format:  # If anything remains, it's invalid\n                    return False, f\"Invalid format pattern in '{placeholder_name}:{format_spec}' - unknown characters: '{temp_format}'\"\n\n        # Validate characters in the entire path (excluding template placeholders)\n        # Remove placeholders temporarily for validation\n        path_without_placeholders = re.sub(placeholder_pattern, 'X', template)\n\n        # Check for invalid characters (only allow alphanumeric, underscore, hyphen, forward slash)\n        if not re.match(r'^[a-zA-Z0-9_\\-/]+$', path_without_placeholders):\n            invalid_chars = re.findall(r'[^a-zA-Z0-9_\\-/]', path_without_placeholders)\n            return False, f\"Folder path contains invalid characters: {set(invalid_chars)} (only letters, numbers, underscore, hyphen, forward slash allowed)\"\n\n        # Check for double slashes\n        if '//' in template:\n            return False, \"Folder path contains double slashes (//)\"\n\n        # Check for leading slash (path should be relative)\n        if template.startswith('/'):\n            return False, \"Folder path should not start with a slash (path is relative to Files/)\"\n\n        return True, None\n\n\n# ============================================================================\n# FILENAME FORMATTER CLASS\n# ============================================================================\n\nclass FilenameFormatter:\n    \"\"\"Process filename templates with parameter substitution and date formatting\n\n    Supports placeholders:\n    - <report_partitioning_column> - parameter name\n    - <report_partitioning_value> - current value\n    - <param_name> - value from static_params\n    - <param_name:yyyymmdd> - formatted date from static_params\n    - <timestamp> or <timestamp:yyyymmdd_hhmmss> - current timestamp\n    \"\"\"\n\n    def __init__(self, template: str, static_params: Dict[str, Any], report_partitioning_column: str, lakehouse_folder_template: str = \"\"):\n        \"\"\"Initialize formatter with template and parameters\"\"\"\n        self.template = template\n        self.static_params = static_params\n        self.report_partitioning_column = report_partitioning_column\n        self.lakehouse_folder_template = lakehouse_folder_template\n        self.parsed_dates = {}\n\n        # Compile regex pattern for placeholders\n        self.placeholder_pattern = re.compile(r'<([^:>]+)(?::([^>]+))?>')\n\n        # Parse and cache dates from static_params\n        self._parse_dates()\n\n        logger.debug(f\"FilenameFormatter initialized with template: {template}\")\n        if lakehouse_folder_template:\n            logger.debug(f\"  Lakehouse folder template: {lakehouse_folder_template}\")\n        logger.debug(f\"  Found {len(self.parsed_dates)} date parameters\")\n\n    def _parse_dates(self):\n        \"\"\"Auto-detect and parse ISO 8601 dates in static_params\"\"\"\n        for key, value in self.static_params.items():\n            if value is None:\n                continue\n\n            value_str = str(value)\n\n            # Try parsing as ISO 8601 date formats\n            date_formats = [\n                '%Y-%m-%d',           # 2025-01-30\n                '%Y-%m-%dT%H:%M:%S',  # 2025-01-30T14:30:22\n                '%Y-%m-%d %H:%M:%S',  # 2025-01-30 14:30:22\n                '%Y%m%d',             # 20250130\n            ]\n\n            for fmt in date_formats:\n                try:\n                    parsed = datetime.strptime(value_str, fmt)\n                    self.parsed_dates[key] = parsed\n                    logger.debug(f\"  Parsed '{key}' as date: {parsed}\")\n                    break\n                except ValueError:\n                    continue\n\n    def _format_date(self, dt: datetime, format_spec: str) -> str:\n        \"\"\"Apply custom date format pattern\n\n        Supported patterns:\n        - yyyy: 4-digit year\n        - yy: 2-digit year\n        - mm: 2-digit month\n        - dd: 2-digit day\n        - ww: ISO week number\n        - hh: 2-digit hour\n        - MM: 2-digit minute (capital M to distinguish from month)\n        - ss: 2-digit second\n        \"\"\"\n        result = format_spec\n\n        # Replace patterns\n        result = result.replace('yyyy', dt.strftime('%Y'))\n        result = result.replace('yy', dt.strftime('%y'))\n        result = result.replace('mm', dt.strftime('%m'))\n        result = result.replace('dd', dt.strftime('%d'))\n        result = result.replace('ww', dt.strftime('%V'))  # ISO week number\n        result = result.replace('hh', dt.strftime('%H'))\n        result = result.replace('MM', dt.strftime('%M'))  # Minutes\n        result = result.replace('ss', dt.strftime('%S'))\n\n        return result\n\n    def _sanitize_component(self, text: str, max_length: int = 100) -> str:\n        \"\"\"Sanitize individual filename component\"\"\"\n        if not text:\n            return \"\"\n\n        text = str(text)\n        # Normalize Unicode\n        text = unicodedata.normalize('NFKD', text)\n        text = text.encode('ascii', 'ignore').decode('ascii')\n        # Remove non-alphanumeric except underscore and hyphen\n        text = re.sub(r'[^\\w\\s-]', '', text)\n        # Collapse whitespace\n        text = re.sub(r'\\s+', '_', text)\n        # Remove leading/trailing separators\n        text = text.strip('_-')\n        # Enforce length\n        if len(text) > max_length:\n            text = text[:max_length]\n\n        return text\n\n    def format(self, report_partitioning_value: str, output_format: str) -> str:\n        \"\"\"Generate filename from template for specific parameter value\n\n        Args:\n            report_partitioning_value: Current value of the partitioning parameter\n            output_format: File extension (PDF, XLSX, etc.)\n\n        Returns:\n            Sanitized filename with extension\n        \"\"\"\n        timestamp = datetime.now(timezone.utc)\n        result = self.template\n\n        # Track what was replaced for logging\n        replacements = {}\n\n        def replace_placeholder(match):\n            \"\"\"Replace a single placeholder\"\"\"\n            placeholder_name = match.group(1)\n            format_spec = match.group(2)\n\n            # Handle special built-in placeholders\n            if placeholder_name == 'report_partitioning_column':\n                value = self.report_partitioning_column\n            elif placeholder_name == 'report_partitioning_value':\n                value = report_partitioning_value\n            elif placeholder_name == 'timestamp':\n                if format_spec:\n                    value = self._format_date(timestamp, format_spec)\n                else:\n                    value = timestamp.strftime('%Y%m%d_%H%M%S')\n            else:\n                # Look up in static_params\n                if placeholder_name not in self.static_params:\n                    # Parameter not found - skip this placeholder (remove it)\n                    logger.debug(f\"  Skipping placeholder <{placeholder_name}> - not found in static_params\")\n                    return ''\n\n                param_value = self.static_params[placeholder_name]\n\n                # Check if this is a date parameter and format requested\n                if format_spec and placeholder_name in self.parsed_dates:\n                    value = self._format_date(self.parsed_dates[placeholder_name], format_spec)\n                else:\n                    value = str(param_value) if param_value is not None else ''\n\n            # Sanitize the value\n            sanitized = self._sanitize_component(value)\n            replacements[placeholder_name] = sanitized\n            return sanitized\n\n        # Replace all placeholders\n        result = self.placeholder_pattern.sub(replace_placeholder, result)\n\n        # Log replacements\n        if replacements:\n            logger.debug(f\"  Template replacements: {replacements}\")\n\n        # Clean up any double underscores or hyphens from removed placeholders\n        result = re.sub(r'_{2,}', '_', result)\n        result = re.sub(r'-{2,}', '-', result)\n        result = result.strip('_-')\n\n        # If result is empty or too short, use fallback\n        if not result or len(result) < 3:\n            result = f\"Report_{self._sanitize_component(report_partitioning_value)}_{timestamp.strftime('%Y%m%d_%H%M%S')}\"\n            logger.warning(f\"Template produced empty/short result, using fallback: {result}\")\n\n        # Add extension\n        extension = output_format.lower()\n        filename = f\"{result}.{extension}\"\n\n        # Final length check (filesystem limit is 255)\n        if len(filename) > 255:\n            # Truncate the base name\n            max_base_length = 255 - len(extension) - 1  # -1 for the dot\n            result = result[:max_base_length]\n            filename = f\"{result}.{extension}\"\n            logger.warning(f\"Filename truncated to {len(filename)} characters\")\n\n        return filename\n\n    def _sanitize_folder_component(self, text: str, max_length: int = 100) -> str:\n        \"\"\"Sanitize individual folder path component (preserves folder structure)\"\"\"\n        if not text:\n            return \"\"\n\n        text = str(text)\n        # Normalize Unicode\n        text = unicodedata.normalize('NFKD', text)\n        text = text.encode('ascii', 'ignore').decode('ascii')\n        # Remove non-alphanumeric except underscore, hyphen, and forward slash\n        text = re.sub(r'[^\\w\\s\\-/]', '', text)\n        # Collapse whitespace to underscore\n        text = re.sub(r'\\s+', '_', text)\n        # Remove leading/trailing separators (but not slashes in the middle)\n        text = text.strip('_-')\n        # Enforce length\n        if len(text) > max_length:\n            text = text[:max_length]\n\n        return text\n\n    def format_folder_path(self, report_partitioning_value: str) -> str:\n        \"\"\"Generate folder path from template for specific parameter value\n\n        Args:\n            report_partitioning_value: Current value of the partitioning parameter\n\n        Returns:\n            Sanitized folder path (without Files/ prefix or trailing slash)\n        \"\"\"\n        timestamp = datetime.now(timezone.utc)\n        result = self.lakehouse_folder_template\n\n        # Track what was replaced for logging\n        replacements = {}\n\n        def replace_placeholder(match):\n            \"\"\"Replace a single placeholder\"\"\"\n            placeholder_name = match.group(1)\n            format_spec = match.group(2)\n\n            # Handle special built-in placeholders\n            if placeholder_name == 'report_partitioning_column':\n                value = self.report_partitioning_column\n            elif placeholder_name == 'report_partitioning_value':\n                value = report_partitioning_value\n            elif placeholder_name == 'timestamp':\n                if format_spec:\n                    value = self._format_date(timestamp, format_spec)\n                else:\n                    value = timestamp.strftime('%Y%m%d_%H%M%S')\n            else:\n                # Look up in static_params\n                if placeholder_name not in self.static_params:\n                    # Parameter not found - skip this placeholder (remove it)\n                    logger.debug(f\"  Skipping placeholder <{placeholder_name}> in folder path - not found in static_params\")\n                    return ''\n\n                param_value = self.static_params[placeholder_name]\n\n                # Check if this is a date parameter and format requested\n                if format_spec and placeholder_name in self.parsed_dates:\n                    value = self._format_date(self.parsed_dates[placeholder_name], format_spec)\n                else:\n                    value = str(param_value) if param_value is not None else ''\n\n            # Sanitize the value (but preserve slashes for folder structure)\n            sanitized = self._sanitize_folder_component(value)\n            replacements[placeholder_name] = sanitized\n            return sanitized\n\n        # Replace all placeholders\n        result = self.placeholder_pattern.sub(replace_placeholder, result)\n\n        # Log replacements\n        if replacements:\n            logger.debug(f\"  Folder path template replacements: {replacements}\")\n\n        # Clean up any double slashes\n        result = re.sub(r'/+', '/', result)\n\n        # Clean up any double underscores or hyphens\n        result = re.sub(r'_{2,}', '_', result)\n        result = re.sub(r'-{2,}', '-', result)\n\n        # Remove leading/trailing separators\n        result = result.strip('_-/')\n\n        return result\n\n\n# ============================================================================\n# TOKEN MANAGER CLASS\n# ============================================================================\n\nclass TokenManager:\n    \"\"\"Manage Power BI API tokens with automatic refresh for long-running batches\n\n    Power BI tokens typically expire after 1 hour. This class tracks token age\n    and refreshes proactively to prevent mid-batch authentication failures.\n    \"\"\"\n\n    def __init__(self, mssparkutils, refresh_interval_minutes: int = 45):\n        \"\"\"Initialize token manager\"\"\"\n        self.mssparkutils = mssparkutils\n        self.refresh_interval = timedelta(minutes=refresh_interval_minutes)\n        self.powerbi_token = None\n        self.token_acquired_at = None\n        self.refresh_tokens()  # Acquire initial tokens\n\n    def refresh_tokens(self) -> Tuple[str, Dict[str, str]]:\n        \"\"\"Refresh Power BI API tokens\"\"\"\n        try:\n            self.powerbi_token = self.mssparkutils.credentials.getToken(\"pbi\")\n            self.token_acquired_at = datetime.now(timezone.utc)\n            logger.info(\"‚úì Power BI API token refreshed successfully\")\n            return self.powerbi_token, self.get_headers()\n        except Exception as e:\n            logger.error(\"‚ùå Failed to refresh Power BI API token\")\n            raise ValueError(f\"Token refresh failed: {str(e)[:200]}\")\n\n    def get_headers(self) -> Dict[str, str]:\n        \"\"\"Get current API headers with bearer token\"\"\"\n        return {\"Authorization\": f\"Bearer {self.powerbi_token}\", \"Content-Type\": \"application/json\"}\n\n    def ensure_valid_token(self) -> Dict[str, str]:\n        \"\"\"Ensure token is valid, refresh if needed\"\"\"\n        if self.token_acquired_at is None:\n            return self.refresh_tokens()[1]\n        else:\n            time_since_refresh = datetime.now(timezone.utc) - self.token_acquired_at\n            if time_since_refresh >= self.refresh_interval:\n                logger.info(f\"üîÑ Token is {time_since_refresh.total_seconds()/60:.1f} minutes old, refreshing...\")\n                self.refresh_tokens()\n        return self.get_headers()\n\n\n# ============================================================================\n# PARAMETER LOADER CLASS\n# ============================================================================\n\nclass ParameterLoader:\n    \"\"\"Load partitioning parameter values from multiple sources with security and retry logic\"\"\"\n\n    def __init__(self, mssparkutils, spark=None, max_retries: int = 3, retry_delay: int = 5):\n        \"\"\"Initialize parameter loader\"\"\"\n        self.mssparkutils = mssparkutils\n        self.spark = spark\n        self.max_retries = max_retries\n        self.retry_delay = retry_delay\n        self.validator = InputValidator()\n\n    def load(self, source_type: str, **config) -> List[str]:\n        \"\"\"Load parameters with retry logic and deduplication\"\"\"\n        if not self.validator.is_valid_source_type(source_type):\n            raise ValueError(f\"Invalid source type: {source_type}. Must be one of: semantic_model, lakehouse, json, warehouse\")\n\n        last_exception = None\n        for attempt in range(self.max_retries):\n            try:\n                if source_type == \"semantic_model\":\n                    values = self._load_from_semantic_model(**config)\n                elif source_type == \"lakehouse\":\n                    values = self._load_from_lakehouse(**config)\n                elif source_type == \"json\":\n                    values = self._load_from_json(**config)\n                elif source_type == \"warehouse\":\n                    values = self._load_from_warehouse(**config)\n\n                # Deduplicate while preserving order\n                seen = set()\n                unique_values = []\n                duplicates = []\n                for v in values:\n                    if v not in seen:\n                        seen.add(v)\n                        unique_values.append(v)\n                    else:\n                        duplicates.append(v)\n\n                if duplicates:\n                    logger.warning(f\"‚ö† Removed {len(duplicates)} duplicate value(s) from parameter list\")\n                    logger.debug(f\"  Duplicates: {duplicates[:10]}\")\n\n                return unique_values\n\n            except Exception as e:\n                last_exception = e\n                if attempt < self.max_retries - 1:\n                    logger.warning(f\"‚ö† Parameter loading attempt {attempt + 1} failed: {str(e)[:200]}\")\n                    logger.info(f\"  ‚Üí Retrying in {self.retry_delay} seconds...\")\n                    time.sleep(self.retry_delay)\n                else:\n                    logger.error(f\"‚ùå All {self.max_retries} parameter loading attempts failed\")\n                    raise Exception(f\"Failed to load parameters from {source_type} after {self.max_retries} attempts: {str(last_exception)[:500]}\")\n\n    def _load_from_semantic_model(self, workspace_id: str, dataset_id: str, dax_query: str, **kwargs) -> List[str]:\n        \"\"\"Load from Semantic Model (Power BI Dataset) via sempy\"\"\"\n        if not self.validator.is_valid_guid(workspace_id):\n            raise ValueError(f\"Invalid workspace GUID format: {workspace_id}\")\n        if not self.validator.is_valid_guid(dataset_id):\n            raise ValueError(f\"Invalid dataset GUID format: {dataset_id}\")\n        if not dax_query or len(dax_query) > 10000:\n            raise ValueError(\"DAX query must be between 1 and 10000 characters\")\n\n        logger.info(f\"üìä Querying Semantic Model...\")\n        logger.info(f\"   Workspace: {workspace_id[:8]}...\")\n        logger.info(f\"   Dataset: {dataset_id[:8]}...\")\n\n        df = fabric.evaluate_dax(dataset=dataset_id, dax_string=dax_query, workspace=workspace_id)\n        if df.empty:\n            raise ValueError(\"Semantic model query returned no results\")\n\n        column_name = df.columns[0]\n        values = [str(v) for v in df[column_name].tolist() if v is not None and str(v).strip()]\n        logger.info(f\"‚úì Loaded {len(values)} values from Semantic Model\")\n        return values\n\n    def _load_from_lakehouse(self, table: str, category: str, column: str, filter_clause: str = \"\", **kwargs) -> List[str]:\n        \"\"\"Load from Lakehouse Delta table via Spark SQL\"\"\"\n        if self.spark is None:\n            raise Exception(\"Spark session not available for lakehouse source\")\n        if not self.validator.is_valid_sql_identifier(table):\n            raise ValueError(f\"Invalid table name: {table}\")\n        if not self.validator.is_valid_sql_identifier(column):\n            raise ValueError(f\"Invalid column name: {column}\")\n\n        safe_category = self.validator.sanitize_sql_string(category)\n        logger.info(f\"üìä Executing Lakehouse query...\")\n\n        query = f\"SELECT {column} FROM {table} WHERE IsActive = true AND Category = '{safe_category}'\"\n        if filter_clause:\n            dangerous_patterns = [';', '--', '/*', '*/', 'xp_', 'sp_', 'DROP', 'DELETE', 'TRUNCATE', 'ALTER', 'CREATE']\n            if any(pattern.lower() in filter_clause.lower() for pattern in dangerous_patterns):\n                raise ValueError(f\"Filter clause contains potentially dangerous SQL keywords\")\n            query += f\" AND ({filter_clause})\"\n        query += \" ORDER BY SortOrder, ParameterName\"\n\n        df = self.spark.sql(query)\n        values = [str(row[0]) for row in df.collect() if row[0] is not None]\n        logger.info(f\"‚úì Loaded {len(values)} values from Lakehouse\")\n        return values\n\n    def _load_from_json(self, json_values: str, **kwargs) -> List[str]:\n        \"\"\"Load from JSON array\"\"\"\n        logger.info(f\"üìä Loading from JSON array...\")\n        if not json_values or json_values.strip() == \"[]\":\n            raise ValueError(\"JSON parameter values cannot be empty\")\n\n        try:\n            values = json.loads(json_values) if isinstance(json_values, str) else json_values\n        except json.JSONDecodeError as e:\n            raise ValueError(f\"Invalid JSON format: {str(e)}\")\n\n        if not isinstance(values, list) or len(values) == 0:\n            raise ValueError(\"JSON parameter values must be a non-empty array\")\n\n        if len(values) > 1000:\n            logger.warning(f\"‚ö† JSON array has {len(values)} values. Consider using Lakehouse or Warehouse for large lists.\")\n\n        values = [str(v) for v in values if v is not None and str(v).strip()]\n        logger.info(f\"‚úì Loaded {len(values)} values from JSON\")\n        return values\n\n    def _load_from_warehouse(self, warehouse_name: str, table: str, column: str, category: str = \"\", **kwargs) -> List[str]:\n        \"\"\"Load from Warehouse via SQL endpoint\"\"\"\n        import pyodbc\n\n        if not warehouse_name or len(warehouse_name) > 128:\n            raise ValueError(\"Invalid warehouse name\")\n        if not self.validator.is_valid_sql_identifier(table):\n            raise ValueError(f\"Invalid table name: {table}\")\n        if not self.validator.is_valid_sql_identifier(column):\n            raise ValueError(f\"Invalid column name: {column}\")\n\n        logger.info(f\"üìä Querying Warehouse...\")\n        conn = None\n        cursor = None\n\n        try:\n            token = self.mssparkutils.credentials.getToken(\"sql\")\n            token_bytes = token.encode(\"UTF-16-LE\")\n            token_struct = struct.pack(f'<I{len(token_bytes)}s', len(token_bytes), token_bytes)\n            SQL_COPT_SS_ACCESS_TOKEN = 1256\n\n            workspace_name = self.mssparkutils.runtime.context.get('workspaceName')\n            if not workspace_name:\n                raise ValueError(\"Could not determine workspace name from context\")\n\n            conn_str = f\"Driver={{ODBC Driver 18 for SQL Server}};Server={workspace_name}.datawarehouse.fabric.microsoft.com;Database={warehouse_name};Encrypt=yes;TrustServerCertificate=no;\"\n            conn = pyodbc.connect(conn_str, attrs_before={SQL_COPT_SS_ACCESS_TOKEN: token_struct}, timeout=30)\n            cursor = conn.cursor()\n\n            safe_category = self.validator.sanitize_sql_string(category)\n            query = f\"SELECT {column} FROM {table} WHERE IsActive = 1\"\n            if category:\n                query += f\" AND Category = '{safe_category}'\"\n            query += \" ORDER BY SortOrder\"\n\n            cursor.execute(query)\n            values = [str(row[0]) for row in cursor.fetchall() if row[0] is not None]\n            logger.info(f\"‚úì Loaded {len(values)} values from Warehouse\")\n            return values\n\n        except Exception as e:\n            raise Exception(f\"Warehouse query failed: {str(e)[:500]}\")\n        finally:\n            if cursor:\n                try:\n                    cursor.close()\n                except:\n                    pass\n            if conn:\n                try:\n                    conn.close()\n                except:\n                    pass\n\n\n# ============================================================================\n# POWER BI API CLIENT CLASS\n# ============================================================================\n\nclass PowerBIAPIClient:\n    \"\"\"Power BI REST API interactions with retry logic\"\"\"\n\n    def __init__(self, token_manager: TokenManager, config: Dict):\n        \"\"\"Initialize API client\"\"\"\n        self.token_manager = token_manager\n        self.workspace_id = config['workspace_id']\n        self.report_id = config['report_id']\n        self.output_format = config['output_format']\n        self.connection_timeout = config['connection_timeout']\n        self.poll_interval = config['poll_interval']\n        self.export_timeout = config['export_timeout']\n        self.download_chunk_size_mb = config['download_chunk_size_mb']\n        self.file_size_warning_mb = config['file_size_warning_mb']\n        self.download_timeout = config['download_timeout']\n\n    def _handle_api_response(self, response: requests.Response, operation: str) -> None:\n        \"\"\"Handle API response with proper error handling and rate limiting\"\"\"\n        if response.status_code == 429:\n            retry_after = int(response.headers.get('Retry-After', '60'))\n            logger.warning(f\"‚ö† API rate limit hit during {operation}\")\n            logger.info(f\"  Waiting {retry_after} seconds before retry...\")\n            time.sleep(retry_after)\n            raise Exception(f\"Rate limited: {operation}. Retry after {retry_after}s\")\n        elif response.status_code >= 400:\n            error_msg = f\"API error during {operation}: HTTP {response.status_code}\"\n            try:\n                error_detail = response.json().get('error', {}).get('message', response.text[:200])\n                error_msg += f\" - {error_detail}\"\n            except:\n                error_msg += f\" - {response.text[:200]}\"\n            raise Exception(error_msg)\n\n    def initiate_export(self, parameters: Dict[str, Any]) -> str:\n        \"\"\"Initiate paginated report export via Power BI REST API\"\"\"\n        export_url = f\"https://api.powerbi.com/v1.0/myorg/groups/{self.workspace_id}/reports/{self.report_id}/ExportTo\"\n        headers = self.token_manager.get_headers()\n        body = {\n            \"format\": self.output_format,\n            \"paginatedReportConfiguration\": {\n                \"parameterValues\": [{\"name\": k, \"value\": str(v)} for k, v in parameters.items()]\n            }\n        }\n        response = requests.post(export_url, headers=headers, json=body, timeout=self.connection_timeout)\n        self._handle_api_response(response, \"initiate export\")\n        return response.json()['id']\n\n    def poll_status(self, export_id: str) -> bool:\n        \"\"\"Poll export status until completion or timeout\"\"\"\n        status_url = f\"https://api.powerbi.com/v1.0/myorg/groups/{self.workspace_id}/reports/{self.report_id}/exports/{export_id}\"\n        headers = self.token_manager.get_headers()\n        start_time = time.time()\n        poll_count = 0\n\n        while time.time() - start_time < self.export_timeout:\n            poll_count += 1\n            try:\n                response = requests.get(status_url, headers=headers, timeout=self.connection_timeout)\n                self._handle_api_response(response, \"poll export status\")\n                status_data = response.json()\n                status = status_data.get('status')\n\n                if status == 'Succeeded':\n                    logger.debug(f\"Export succeeded after {poll_count} polls ({time.time() - start_time:.1f}s)\")\n                    return True\n                elif status == 'Failed':\n                    error = status_data.get('error', {}).get('message', 'Unknown error')\n                    raise Exception(f\"Export failed: {error}\")\n                elif status in ['Running', 'NotStarted']:\n                    time.sleep(self.poll_interval)\n                else:\n                    logger.warning(f\"‚ö† Unknown export status: {status}\")\n                    time.sleep(self.poll_interval)\n            except Exception as e:\n                if \"Rate limited\" in str(e):\n                    raise\n                logger.warning(f\"‚ö† Error during status poll {poll_count}: {str(e)[:200]}\")\n                time.sleep(self.poll_interval)\n\n        raise TimeoutError(f\"Export timeout after {self.export_timeout} seconds ({poll_count} polls)\")\n\n    def download_file(self, export_id: str) -> bytes:\n        \"\"\"Download exported report file\"\"\"\n        file_url = f\"https://api.powerbi.com/v1.0/myorg/groups/{self.workspace_id}/reports/{self.report_id}/exports/{export_id}/file\"\n        headers = self.token_manager.get_headers()\n        response = requests.get(file_url, headers=headers, stream=True, timeout=self.download_timeout)\n        self._handle_api_response(response, \"download report file\")\n\n        content = b''\n        chunk_size_bytes = self.download_chunk_size_mb * 1024 * 1024\n        file_size_warning_bytes = self.file_size_warning_mb * 1024 * 1024\n\n        for chunk in response.iter_content(chunk_size=chunk_size_bytes):\n            if chunk:\n                content += chunk\n                if len(content) > file_size_warning_bytes:\n                    logger.warning(f\"‚ö† Report file exceeds {self.file_size_warning_mb}MB, may cause memory issues\")\n\n        return content\n\n\n# ============================================================================\n# ONELAKE STORAGE CLASS\n# ============================================================================\n\nclass OneLakeStorage:\n    \"\"\"File storage operations with flexible filename formatting and date-based hierarchy\"\"\"\n\n    def __init__(self, mssparkutils, config: Dict, filename_formatter: FilenameFormatter):\n        \"\"\"Initialize storage handler\"\"\"\n        self.mssparkutils = mssparkutils\n        self.output_format = config['output_format']\n        self.file_size_warning_mb = config['file_size_warning_mb']\n        self.filename_formatter = filename_formatter\n        self.lakehouse_folder = config.get('lakehouse_folder', '')\n\n    def save(self, file_content: bytes, report_partitioning_value: str) -> str:\n        \"\"\"Save binary file to OneLake for archival with date-based organization\n\n        This function properly handles binary content (PDF, XLSX, etc.) by:\n        1. Generating filename using FilenameFormatter\n        2. Writing to a temporary file first\n        3. Copying to OneLake using mssparkutils.fs.cp()\n        4. Cleaning up the temporary file\n\n        Note: Requires a Lakehouse to be attached to the notebook as a data item.\n        \"\"\"\n        timestamp = datetime.now(timezone.utc)\n\n        # Use formatter to generate filename\n        filename = self.filename_formatter.format(report_partitioning_value, self.output_format)\n        logger.debug(f\"  Generated filename: {filename}\")\n\n        # Determine folder path\n        if self.lakehouse_folder:\n            # User specified custom folder path - format it\n            folder_path = self.filename_formatter.format_folder_path(report_partitioning_value)\n            # Prepend Files/ (path is relative to Files/ directory)\n            onelake_path = f\"Files/{folder_path}/{filename}\"\n            logger.debug(f\"  Using custom folder: {folder_path}\")\n        else:\n            # Use default: Files/reports/archive/YYYY/MM/DD/\n            date_path = timestamp.strftime('%Y/%m/%d')\n            onelake_path = f\"Files/reports/archive/{date_path}/{filename}\"\n            logger.debug(f\"  Using default folder structure: reports/archive/{date_path}\")\n\n        file_size_mb = len(file_content) / (1024 * 1024)\n        if file_size_mb > self.file_size_warning_mb:\n            logger.warning(f\"‚ö† File size is {file_size_mb:.1f}MB, which is very large\")\n\n        temp_file = None\n        temp_path = None\n        try:\n            # Create a temporary file to write binary content\n            with tempfile.NamedTemporaryFile(delete=False, suffix=f\".{self.output_format.lower()}\") as temp_file:\n                temp_file.write(file_content)\n                temp_path = temp_file.name\n\n            # Copy from temp file to OneLake using file:// protocol\n            temp_url = f\"file://{temp_path}\"\n\n            # Delete existing file if it exists (since cp doesn't support overwrite parameter)\n            try:\n                self.mssparkutils.fs.rm(onelake_path)\n            except Exception:\n                pass  # File doesn't exist, that's fine\n\n            # Use mssparkutils.fs.cp to copy the file to OneLake\n            self.mssparkutils.fs.cp(temp_url, onelake_path)\n\n            logger.debug(f\"  Saved {file_size_mb:.2f}MB to OneLake\")\n\n        except Exception as e:\n            error_msg = str(e)\n            # Don't include binary content in error message\n            if len(error_msg) > 200:\n                error_msg = error_msg[:200]\n            raise Exception(f\"Failed to write to OneLake: {error_msg}\")\n        finally:\n            # Clean up temporary file\n            if temp_path and os.path.exists(temp_path):\n                try:\n                    os.unlink(temp_path)\n                except Exception:\n                    pass  # Ignore cleanup errors\n\n        return onelake_path\n\n\n# ============================================================================\n# PAGINATED REPORT EXECUTOR CLASS (Main Orchestrator)\n# ============================================================================\n\nclass PaginatedReportExecutor:\n    \"\"\"Main orchestrator for paginated report batch execution using composition\"\"\"\n\n    def __init__(self, config: Dict, mssparkutils, spark=None):\n        \"\"\"Initialize executor and all components\"\"\"\n        self.config = config\n        self.mssparkutils = mssparkutils\n        self.spark = spark\n\n        # Initialize logger\n        logger.info(\"\\n\" + \"=\"*60)\n        logger.info(\"INITIALIZING PAGINATED REPORT EXECUTOR\")\n        logger.info(\"=\"*60 + \"\\n\")\n\n        # Compose helper objects\n        self.validator = InputValidator()\n\n        # Validate filename template early\n        is_valid, error_msg = self.validator.validate_filename_template(config.get('report_name', ''))\n        if not is_valid:\n            raise ValueError(f\"Invalid filename template: {error_msg}\")\n        logger.info(f\"‚úì Filename template validated\")\n\n        # Validate lakehouse folder template if provided\n        lakehouse_folder_val = config.get('lakehouse_folder', '')\n        if lakehouse_folder_val:\n            is_valid, error_msg = self.validator.validate_lakehouse_folder_template(lakehouse_folder_val)\n            if not is_valid:\n                raise ValueError(f\"Invalid lakehouse_folder template: {error_msg}\")\n            logger.info(f\"‚úì Lakehouse folder template validated: {lakehouse_folder_val}\")\n        else:\n            logger.info(f\"‚úì Using default lakehouse folder structure\")\n\n        # Create filename formatter (parse dates once at initialization)\n        self.filename_formatter = FilenameFormatter(\n            template=config['report_name'],\n            static_params=config['static_params'],\n            report_partitioning_column=config['report_partitioning_column'],\n            lakehouse_folder_template=lakehouse_folder_val\n        )\n        logger.info(f\"‚úì Filename formatter created\")\n\n        self.token_manager = TokenManager(\n            mssparkutils,\n            refresh_interval_minutes=config['token_refresh_interval']\n        )\n        self.param_loader = ParameterLoader(\n            mssparkutils,\n            spark,\n            max_retries=config['param_loader_max_retries'],\n            retry_delay=config['param_loader_retry_delay']\n        )\n        self.api_client = PowerBIAPIClient(self.token_manager, config)\n\n        # Storage is optional (only if archiving enabled)\n        if config['archive_to_onelake']:\n            self.storage = OneLakeStorage(mssparkutils, config, self.filename_formatter)\n        else:\n            self.storage = None\n\n        # Validate all parameters\n        self._validate_all_parameters()\n\n        # Log configuration\n        self._log_configuration()\n\n        logger.info(\"‚úì Executor initialized successfully\\n\")\n\n    def _validate_all_parameters(self):\n        \"\"\"Validate all configuration parameters\"\"\"\n        if not self.validator.is_valid_guid(self.config['workspace_id']):\n            raise ValueError(f\"Invalid workspace_id format. Must be a valid GUID. Received: '{self.config['workspace_id']}'\")\n        if not self.validator.is_valid_guid(self.config['report_id']):\n            raise ValueError(f\"Invalid report_id format. Must be a valid GUID. Received: '{self.config['report_id']}'\")\n        if not self.validator.is_valid_format(self.config['output_format']):\n            raise ValueError(f\"Invalid output_format: {self.config['output_format']}. Must be one of: PDF, XLSX, DOCX, PPTX, PNG\")\n        if not self.validator.is_valid_source_type(self.config['report_partitioning_source']):\n            raise ValueError(f\"Invalid report_partitioning_source: {self.config['report_partitioning_source']}. Must be one of: semantic_model, lakehouse, json, warehouse\")\n\n        # Validate source-specific parameters based on selected source\n        logger.info(f\"üìã Validating configuration for source type: {self.config['report_partitioning_source']}\")\n\n        source_type = self.config['report_partitioning_source']\n        if source_type == \"semantic_model\":\n            source_config = self.config['semantic_model']\n            if not self.validator.is_valid_guid(source_config['workspace_id']):\n                raise ValueError(f\"Invalid semantic_model workspace_id format. Required for semantic_model source.\")\n            if not self.validator.is_valid_guid(source_config['dataset_id']):\n                raise ValueError(f\"Invalid semantic_model dataset_id format. Required for semantic_model source.\")\n            if not source_config['dax_query']:\n                raise ValueError(\"semantic_model_dax_query is required for semantic_model source\")\n            logger.info(f\"‚úì Semantic Model configuration validated\")\n\n        elif source_type == \"lakehouse\":\n            source_config = self.config['lakehouse']\n            if not source_config['table']:\n                raise ValueError(\"lakehouse_table is required for lakehouse source\")\n            if not source_config['category']:\n                raise ValueError(\"lakehouse_category is required for lakehouse source\")\n            if not source_config['column']:\n                raise ValueError(\"lakehouse_column is required for lakehouse source\")\n            logger.info(f\"‚úì Lakehouse configuration validated\")\n\n        elif source_type == \"json\":\n            source_config = self.config['json']\n            if not source_config['json_values'] or source_config['json_values'].strip() == \"[]\":\n                raise ValueError(\"report_partitioning_values is required for json source and cannot be empty\")\n            logger.info(f\"‚úì JSON configuration validated\")\n\n        elif source_type == \"warehouse\":\n            source_config = self.config['warehouse']\n            if not source_config['warehouse_name']:\n                raise ValueError(\"warehouse_name is required for warehouse source\")\n            if not source_config['table']:\n                raise ValueError(\"warehouse_table is required for warehouse source\")\n            if not source_config['column']:\n                raise ValueError(\"warehouse_column is required for warehouse source\")\n            logger.info(f\"‚úì Warehouse configuration validated\")\n\n        # Validate static parameters\n        if not isinstance(self.config['static_params'], dict):\n            raise ValueError(\"static_params must be a dictionary\")\n        logger.info(f\"‚úì Static parameters validated: {len(self.config['static_params'])} parameter(s)\")\n\n        logger.info(\"‚úì All configuration validated successfully\")\n\n    def _log_configuration(self):\n        \"\"\"Log startup configuration\"\"\"\n        logger.info(\"Configuration:\")\n        logger.info(f\"  Report ID: {self.config['report_id'][:8] if self.config['report_id'] else 'Not set'}...\")\n        logger.info(f\"  Partitioning parameter: {self.config['report_partitioning_column']}\")\n        logger.info(f\"  Source: {self.config['report_partitioning_source']}\")\n        logger.info(f\"  Output format: {self.config['output_format']}\")\n        logger.info(f\"  Filename template: {self.config['report_name']}\")\n        logger.info(f\"  Lakehouse folder: {self.config.get('lakehouse_folder', '(default)')}\")\n        logger.info(f\"  Archive to OneLake: {self.config['archive_to_onelake']}\")\n        logger.info(f\"  Export timeout: {self.config['export_timeout']}s\")\n        logger.info(f\"  Max retries: {self.config['max_retries']}\")\n        logger.info(f\"  Token refresh interval: {self.config['token_refresh_interval']} minutes\")\n\n    def _load_partitioning_values(self) -> List[str]:\n        \"\"\"Load partitioning parameter values from configured source\"\"\"\n        logger.info(\"\\n\" + \"=\"*60)\n        logger.info(\"LOADING PARTITIONING PARAMETER VALUES\")\n        logger.info(\"=\"*60 + \"\\n\")\n\n        source_type = self.config['report_partitioning_source']\n        source_config = self.config[source_type]\n\n        try:\n            report_partitioning_values = self.param_loader.load(source_type, **source_config)\n        except Exception as e:\n            logger.error(f\"‚ùå Failed to load parameters: {str(e)[:500]}\")\n            logger.error(f\"   Source: {source_type}\")\n            logger.error(f\"   Check your configuration and ensure:\")\n            logger.error(f\"   - Data source exists and is accessible\")\n            logger.error(f\"   - Permissions are granted\")\n            logger.error(f\"   - Parameters are correctly formatted\")\n            raise\n\n        logger.info(f\"\\n{'='*60}\")\n        logger.info(f\"Parameter '{self.config['report_partitioning_column']}' loaded: {len(report_partitioning_values)} unique values\")\n        if len(report_partitioning_values) <= 10:\n            logger.info(f\"Values: {report_partitioning_values}\")\n        else:\n            logger.info(f\"First 10 values: {report_partitioning_values[:10]}\")\n            logger.info(f\"... and {len(report_partitioning_values) - 10} more\")\n        logger.info(f\"{'='*60}\\n\")\n\n        # Validation\n        if not report_partitioning_values or len(report_partitioning_values) == 0:\n            raise ValueError(\n                f\"‚ùå No parameter values loaded from {source_type}! \"\n                f\"Check your configuration:\\n\"\n                f\"  - Ensure the data source has data\\n\"\n                f\"  - Verify category/filter settings\\n\"\n                f\"  - Check permissions\"\n            )\n\n        # Warnings for large batches\n        if len(report_partitioning_values) > 100:\n            estimated_minutes = len(report_partitioning_values) * 2\n            logger.warning(f\"‚ö† Processing {len(report_partitioning_values)} values may take a long time\")\n            logger.warning(f\"  Estimated time: {estimated_minutes} minutes (assuming 2 min per report)\")\n            logger.warning(f\"  Token will auto-refresh every {self.config['token_refresh_interval']} minutes\")\n\n        if len(report_partitioning_values) > 500:\n            logger.warning(f\"‚ö† Very large batch detected! This may take hours to complete.\")\n            logger.warning(f\"  Recommendation: Use multiple pipelines to process in parallel\")\n\n        logger.info(\"‚úì Partitioning parameter values loaded, deduplicated, and validated\")\n\n        return report_partitioning_values\n\n    def _execute_single_report(self, params: Dict, report_partitioning_value: str) -> Dict:\n        \"\"\"Execute report for a single parameter value with retry logic\"\"\"\n        start_time = datetime.now(timezone.utc)\n\n        for attempt in range(self.config['max_retries']):\n            try:\n                # Ensure token is valid (refreshes if needed)\n                self.token_manager.ensure_valid_token()\n\n                # Step 1: Initiate export\n                logger.info(f\"  Step 1/4: Initiating report export...\")\n                export_id = self.api_client.initiate_export(params)\n                logger.info(f\"    ‚úì Export initiated. Export ID: {export_id[:8]}...\")\n\n                # Step 2: Poll for completion\n                logger.info(f\"  Step 2/4: Waiting for export to complete...\")\n                self.api_client.poll_status(export_id)\n                logger.info(f\"    ‚úì Export completed successfully\")\n\n                # Step 3: Download file\n                logger.info(f\"  Step 3/4: Downloading report file...\")\n                file_content = self.api_client.download_file(export_id)\n                file_size_mb = len(file_content) / (1024 * 1024)\n                logger.info(f\"    ‚úì Downloaded {file_size_mb:.2f} MB\")\n\n                # Step 4: Save to OneLake (if enabled)\n                onelake_path = None\n                if self.storage:\n                    logger.info(f\"  Step 4/4: Saving to OneLake archive...\")\n                    onelake_path = self.storage.save(file_content, report_partitioning_value)\n                    logger.info(f\"    ‚úì Saved to: {onelake_path}\")\n\n                # Success!\n                end_time = datetime.now(timezone.utc)\n                duration = (end_time - start_time).total_seconds()\n\n                return {\n                    'partitioning_value': report_partitioning_value,\n                    'status': 'SUCCESS',\n                    'onelake_path': onelake_path,\n                    'file_size_mb': round(file_size_mb, 2),\n                    'duration_seconds': round(duration, 2),\n                    'timestamp': end_time.isoformat(),\n                    'attempts': attempt + 1,\n                    'error': None\n                }\n\n            except Exception as e:\n                error_msg = str(e)[:500]\n\n                if attempt < self.config['max_retries'] - 1:\n                    wait_time = self.config['retry_backoff_base'] * (2 ** attempt)\n                    logger.warning(f\"  ‚úó Attempt {attempt + 1}/{self.config['max_retries']} failed: {error_msg}\")\n                    logger.info(f\"  ‚Üí Retrying in {wait_time} seconds...\")\n                    time.sleep(wait_time)\n                else:\n                    # All retries exhausted\n                    end_time = datetime.now(timezone.utc)\n                    duration = (end_time - start_time).total_seconds()\n                    logger.error(f\"  ‚úó All {self.config['max_retries']} attempts failed for '{partitioning_value}'\")\n                    logger.error(f\"     Final error: {error_msg}\")\n\n                    return {\n                        'partitioning_value': report_partitioning_value,\n                        'status': 'FAILED',\n                        'onelake_path': None,\n                        'file_size_mb': 0,\n                        'duration_seconds': round(duration, 2),\n                        'timestamp': end_time.isoformat(),\n                        'attempts': self.config['max_retries'],\n                        'error': error_msg\n                    }\n\n        # Should never reach here\n        return {\n            'partitioning_value': report_partitioning_value,\n            'status': 'FAILED',\n            'error': 'Unknown error - retry loop completed unexpectedly',\n            'attempts': self.config['max_retries']\n        }\n\n    def _generate_summary(self, results: List[Dict], total_duration: float) -> Dict:\n        \"\"\"Generate execution summary and pipeline result JSON\"\"\"\n        success_count = len([r for r in results if r['status'] == 'SUCCESS'])\n        fail_count = len(results) - success_count\n        total_size_mb = sum(r.get('file_size_mb', 0) for r in results)\n        avg_duration = sum(r.get('duration_seconds', 0) for r in results) / len(results) if results else 0\n        total_attempts = sum(r.get('attempts', 1) for r in results)\n\n        logger.info(f\"\\n{'='*60}\")\n        logger.info(\"EXECUTION SUMMARY\")\n        logger.info(f\"{'='*60}\")\n        logger.info(f\"Total reports processed: {len(results)}\")\n        logger.info(f\"‚úì Successful: {success_count}\")\n        logger.info(f\"‚úó Failed: {fail_count}\")\n        logger.info(f\"Total size: {total_size_mb:.2f} MB\")\n        logger.info(f\"Average duration per report: {avg_duration:.1f} seconds\")\n        logger.info(f\"Success rate: {(success_count/len(results)*100):.1f}%\")\n        logger.info(f\"Total retry attempts: {total_attempts} (avg {total_attempts/len(results):.1f} per report)\")\n\n        # Print successful files\n        if success_count > 0:\n            logger.info(f\"\\n{'='*60}\")\n            logger.info(f\"GENERATED FILES (Saved to OneLake): {success_count} files\")\n            logger.info(f\"{'='*60}\")\n            # Show first 20, then summarize\n            for idx, r in enumerate([r for r in results if r['status'] == 'SUCCESS'][:20], 1):\n                logger.info(f\"  {idx}. {r['onelake_path']} ({r['file_size_mb']} MB)\")\n            if success_count > 20:\n                logger.info(f\"  ... and {success_count - 20} more files\")\n\n        # Print failures\n        if fail_count > 0:\n            logger.info(f\"\\n{'='*60}\")\n            logger.error(f\"FAILURES: {fail_count} reports failed\")\n            logger.info(f\"{'='*60}\")\n            for idx, r in enumerate([r for r in results if r['status'] == 'FAILED'], 1):\n                # Truncate error message for readability\n                error_msg = r.get('error', 'Unknown error')\n                if len(error_msg) > 200:\n                    error_msg = error_msg[:200] + \"...\"\n                logger.error(f\"  {idx}. '{r['partitioning_value']}': {error_msg}\")\n\n        completion_time = datetime.now(timezone.utc)\n        logger.info(f\"\\n{'='*60}\")\n        logger.info(f\"Execution completed at: {completion_time.strftime('%Y-%m-%d %H:%M:%S UTC')}\")\n        logger.info(f\"{'='*60}\\n\")\n\n        # Prepare file list for pipeline consumption\n        successful_files = [\n            r['onelake_path']\n            for r in results\n            if r['status'] == 'SUCCESS' and r.get('onelake_path')\n        ]\n\n        # Prepare result for pipeline (must match pipeline's expected JSON structure)\n        pipeline_result = {\n            'files': successful_files,\n            'status': 'success' if fail_count == 0 else 'partial_success' if success_count > 0 else 'failed',\n            'total': len(results),\n            'success_count': success_count,\n            'fail_count': fail_count,\n            'total_size_mb': round(total_size_mb, 2),\n            'avg_duration_seconds': round(avg_duration, 2),\n            'total_duration_seconds': round(total_duration, 2),\n            'errors': [\n                {'value': r['partitioning_value'], 'error': r['error'][:200] if r.get('error') else 'Unknown'}\n                for r in results if r['status'] == 'FAILED'\n            ][:50],  # Limit to first 50 errors to avoid huge JSON\n            'timestamp': completion_time.isoformat(),\n            'parameter_name': self.config['report_partitioning_column'],\n            'source_type': self.config['report_partitioning_source']\n        }\n\n        logger.info(f\"üì§ Returning result to pipeline:\")\n        logger.info(f\"   Status: {pipeline_result['status']}\")\n        logger.info(f\"   Files: {len(successful_files)} paths\")\n        logger.info(f\"   Success: {success_count}/{len(results)}\")\n\n        return pipeline_result\n\n    def execute_batch(self) -> Dict:\n        \"\"\"Main execution method - PUBLIC API\"\"\"\n        logger.info(\"\\n\" + \"=\"*60)\n        logger.info(\"STARTING REPORT BATCH EXECUTION\")\n        logger.info(\"=\"*60 + \"\\n\")\n        sys.stdout.flush()\n\n        execution_start_time = datetime.now(timezone.utc)\n\n        print(f\"\\n‚è± Execution started at: {execution_start_time}\")\n        sys.stdout.flush()\n\n        # Load partitioning parameter values\n        report_partitioning_values = self._load_partitioning_values()\n\n        print(f\"üìä Total items to process: {len(report_partitioning_values)}\")\n        print(f\"üîÑ Starting loop...\")\n        sys.stdout.flush()\n\n        # Execute batch loop\n        results = []\n        for idx, partitioning_value in enumerate(report_partitioning_values, 1):\n            # IMMEDIATE FEEDBACK - Print to stdout AND logger\n            print(f\"\\n{'='*60}\")\n            print(f\"‚ñ∂ PROCESSING {idx}/{len(report_partitioning_values)}: {self.config['report_partitioning_column']} = '{partitioning_value}'\")\n            print(f\"{'='*60}\")\n            sys.stdout.flush()\n\n            logger.info(f\"\\n{'='*60}\")\n            logger.info(f\"Processing {idx}/{len(report_partitioning_values)}: {self.config['report_partitioning_column']} = '{partitioning_value}'\")\n            logger.info(f\"{'='*60}\\n\")\n            sys.stdout.flush()\n\n            # Merge static parameters with current special value\n            print(f\"  ‚öô Merging parameters...\")\n            sys.stdout.flush()\n\n            all_params = self.config['static_params'].copy()\n            all_params[self.config['report_partitioning_column']] = partitioning_value\n\n            print(f\"  ‚úì Parameters merged\")\n            logger.info(f\"  Parameters:\")\n            for key, value in all_params.items():\n                value_str = str(value)\n                if len(value_str) > 100:\n                    value_str = value_str[:100] + \"...\"\n                logger.info(f\"    {key}: {value_str}\")\n            logger.info(\"\")\n            sys.stdout.flush()\n\n            # Execute report with retry logic\n            print(f\"  üöÄ Calling execute_report_with_retry...\")\n            sys.stdout.flush()\n\n            result = self._execute_single_report(all_params, partitioning_value)\n\n            print(f\"  ‚úì execute_report_with_retry returned\")\n            sys.stdout.flush()\n\n            results.append(result)\n\n            # Print result summary with immediate flush\n            if result['status'] == 'SUCCESS':\n                print(f\"  ‚úÖ SUCCESS ({result['duration_seconds']}s, {result['attempts']} attempt(s))\")\n                sys.stdout.flush()\n                logger.info(f\"\\n  ‚úÖ SUCCESS ({result['duration_seconds']}s, {result['attempts']} attempt(s))\")\n                logger.info(f\"     OneLake: {result['onelake_path']}\")\n                logger.info(f\"     Size: {result['file_size_mb']} MB\")\n            else:\n                print(f\"  ‚ùå FAILED ({result['duration_seconds']}s, {result['attempts']} attempt(s))\")\n                print(f\"     Error: {result['error']}\")\n                sys.stdout.flush()\n                logger.error(f\"\\n  ‚ùå FAILED ({result['duration_seconds']}s, {result['attempts']} attempt(s))\")\n                logger.error(f\"     Error: {result['error']}\")\n\n            sys.stdout.flush()\n\n            # Progress update for large batches\n            if len(report_partitioning_values) > 20 and idx % 10 == 0:\n                success_so_far = len([r for r in results if r['status'] == 'SUCCESS'])\n                pct_complete = (idx / len(report_partitioning_values)) * 100\n                print(f\"\\n  üìä Progress: {idx}/{len(report_partitioning_values)} ({pct_complete:.1f}%) - {success_so_far} successful\")\n                sys.stdout.flush()\n                logger.info(f\"\\n  üìä Progress: {idx}/{len(report_partitioning_values)} ({pct_complete:.1f}%) - {success_so_far} successful\")\n\n        print(\"\\n\" + \"=\" * 60)\n        print(\"LOOP COMPLETED\")\n        print(\"=\" * 60)\n        sys.stdout.flush()\n\n        execution_end_time = datetime.now(timezone.utc)\n        total_duration = (execution_end_time - execution_start_time).total_seconds()\n\n        logger.info(f\"\\n{'='*60}\")\n        logger.info(\"EXECUTION COMPLETE\")\n        logger.info(f\"{'='*60}\")\n        logger.info(f\"Total time: {total_duration:.1f} seconds ({total_duration/60:.1f} minutes)\")\n        logger.info(f\"Average per report: {total_duration/len(results):.1f} seconds\")\n        sys.stdout.flush()\n\n        print(f\"\\n‚úÖ BATCH EXECUTION COMPLETED SUCCESSFULLY\")\n        print(f\"   Processed: {len(results)} reports\")\n        print(f\"   Duration: {total_duration:.1f} seconds\")\n        sys.stdout.flush()\n\n        # Generate and return summary\n        return self._generate_summary(results, total_duration)\n\n\n# ============================================================================\n# MAIN EXECUTION\n# ============================================================================\n\nif __name__ == \"__main__\":\n    logger.info(f\"\\n{'='*60}\")\n    logger.info(\"PAGINATED REPORT BATCH EXECUTOR v1.0\")\n    logger.info(f\"Started at: {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')}\")\n    logger.info(f\"{'='*60}\\n\")\n\n    # Create executor with all configuration\n    executor = PaginatedReportExecutor(\n        config=config,\n        mssparkutils=mssparkutils,\n        spark=spark\n    )\n\n    # Execute batch\n    result = executor.execute_batch()\n\n    # Exit for pipeline integration\n    logger.info(\"\\nüì§ Exiting notebook with result for pipeline...\")\n    mssparkutils.notebook.exit(json.dumps(result))\n\n    logger.info(\"‚úì Notebook completed successfully\")\n"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}