{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Publish and Distribute Paginated Reports\n",
    "\n",
    "This notebook executes a paginated report multiple times with different parameter values and saves the output to OneLake.\n",
    "\n",
    "**Features:**\n",
    "- 4 flexible parameter sources: Semantic Model, Lakehouse, JSON, Warehouse\n",
    "- Automatic token refresh for long-running batches\n",
    "- Configurable performance tuning parameters\n",
    "- OneLake archival with date-based folder hierarchy\n",
    "- Retry logic with exponential backoff\n",
    "- Comprehensive logging and error handling\n",
    "- Input validation and SQL injection protection\n",
    "- Pipeline integration ready\n",
    "\n",
    "**Pipeline Integration:**\n",
    "The notebook exits with JSON containing the file list:\n",
    "```json\n",
    "{\n",
    "  \"files\": [\"Files/reports/archive/2025/01/30/Report_Customer_A.pdf\", ...],\n",
    "  \"status\": \"success\",\n",
    "  \"success_count\": 10,\n",
    "  \"fail_count\": 0\n",
    "}\n",
    "```\n",
    "\n",
    "Pipeline ForEach items reference: `@json(activity('NotebookActivity').output.status.Output.result.exitValue).files`\n",
    "\n",
    "**Author:** Generated by Claude Code  \n",
    "**Version:** 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": "# CELL 1: Parameter Definitions\n# These parameters can be overridden by pipeline\n\n# Report configuration\nworkspace_id = \"\"                    # Fabric workspace GUID\nreport_id = \"\"                       # Paginated report GUID\noutput_format = \"XLSX\"                # PDF, XLSX, DOCX, PPTX, PNG\nstatic_params = \"{}\"                 # JSON: {\"start_date\": \"2024-01-01\", \"end_date\": \"2024-12-31\"}\nspecial_param_name = \"Producer\"       # Parameter name to loop through\n\n# ============================================================\n# SOURCE CONFIGURATION (supports 4 sources)\n# ============================================================\nspecial_values_source = \"semantic_model\"  # \"semantic_model\" | \"lakehouse\" | \"json\" | \"warehouse\"\n\n# -------------------- OPTION 1: SEMANTIC MODEL (RECOMMENDED) --------------------\nsemantic_model_workspace_id = \"\"       # Workspace containing the semantic model\nsemantic_model_dataset_id = \"\"         # Semantic model (dataset) GUID\nsemantic_model_dax_query = \"\"          # DAX query to get parameter values\n\n# -------------------- OPTION 2: LAKEHOUSE TABLE --------------------\nlakehouse_table = \"parameter_config\"  # Delta table name\nlakehouse_category = \"ProducerList\"   # Category filter\nlakehouse_column = \"ParameterValue\"   # Column containing values\nlakehouse_filter = \"\"                 # Optional: additional WHERE clause\n\n# -------------------- OPTION 3: JSON ARRAY --------------------\nspecial_parameter_values = \"[]\"       # JSON: [\"Producer A\", \"Producer B\", \"Producer C\"]\n\n# -------------------- OPTION 4: WAREHOUSE --------------------\nwarehouse_name = \"\"                   # Warehouse name\nwarehouse_table = \"\"                  # Table name (e.g., \"dbo.ParameterConfig\")\nwarehouse_column = \"\"                 # Column name\nwarehouse_category = \"\"               # Category filter\n\n# ============================================================\n# EXECUTION OPTIONS\n# ============================================================\narchive_to_onelake = \"true\"           # Save to OneLake\nmax_retries = \"3\"                     # Retry attempts per report\nexport_timeout_seconds = \"600\"        # Max seconds to wait for export (10 minutes)\npoll_interval_seconds = \"5\"           # Seconds between status polls\nretry_backoff_base = \"30\"             # Base seconds for exponential backoff (30, 60, 120...)\n\n# ============================================================\n# PERFORMANCE TUNING (OPTIONAL)\n# ============================================================\ndownload_chunk_size_mb = \"1\"          # Download chunk size in MB\nfile_size_warning_mb = \"500\"          # File size warning threshold in MB\nconnection_timeout_seconds = \"30\"     # API connection timeout in seconds\ndownload_timeout_seconds = \"120\"      # Download timeout in seconds\nparam_loader_retry_attempts = \"3\"     # Parameter loading retry attempts\nparam_loader_retry_delay_seconds = \"5\"  # Delay between parameter loading retries\ntoken_refresh_interval_minutes = \"45\"  # Token refresh interval in minutes\n\n# NOTE: Config dictionary is built in Cell 2 AFTER pipeline parameters are injected"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# CELL 2: COMPLETE OOP IMPLEMENTATION\n# ============================================================================\n\n# ============================================================================\n# IMPORTS AND PACKAGE INSTALLATION\n# ============================================================================\n\nimport requests\nimport json\nimport time\nimport re\nimport struct\nimport sys\nimport os\nimport tempfile\nimport unicodedata\nfrom datetime import datetime, timezone, timedelta\nfrom typing import List, Dict, Any, Optional, Tuple\n\ndef install_package(package: str) -> None:\n    \"\"\"Install package if not already installed\"\"\"\n    import subprocess\n    try:\n        __import__(package.replace('-', '_'))\n    except ImportError:\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"-q\"])\n\n# Install semantic-link for semantic model support\ntry:\n    import sempy.fabric as fabric\nexcept ImportError:\n    install_package(\"semantic-link\")\n    import sempy.fabric as fabric\n\n# Fabric imports\nfrom notebookutils import mssparkutils\n\n# ============================================================================\n# LOGGER SETUP\n# ============================================================================\n\nimport logging\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger(__name__)\n\n# ============================================================================\n# BUILD CONFIGURATION DICTIONARY\n# ============================================================================\n# Pipeline parameters are injected between Cell 1 and Cell 2, so we build\n# the config dictionary here to capture the injected values\n\n# Parse JSON parameters\nstatic_params_dict = json.loads(static_params)\n\n# Strip whitespace from GUIDs (in case pipeline adds spaces)\nworkspace_id = workspace_id.strip() if workspace_id else \"\"\nreport_id = report_id.strip() if report_id else \"\"\n\n# Build unified configuration dictionary for executor\nconfig = {\n    # Core report configuration\n    'workspace_id': workspace_id,\n    'report_id': report_id,\n    'output_format': output_format.upper(),\n    'static_params': static_params_dict,\n    'special_param_name': special_param_name,\n    'special_values_source': special_values_source,\n    \n    # Source-specific configurations\n    'semantic_model': {\n        'workspace_id': semantic_model_workspace_id.strip() if semantic_model_workspace_id else \"\",\n        'dataset_id': semantic_model_dataset_id.strip() if semantic_model_dataset_id else \"\",\n        'dax_query': semantic_model_dax_query\n    },\n    'lakehouse': {\n        'table': lakehouse_table,\n        'category': lakehouse_category,\n        'column': lakehouse_column,\n        'filter_clause': lakehouse_filter\n    },\n    'json': {\n        'json_values': special_parameter_values\n    },\n    'warehouse': {\n        'warehouse_name': warehouse_name,\n        'table': warehouse_table,\n        'column': warehouse_column,\n        'category': warehouse_category\n    },\n    \n    # Execution settings (convert strings to appropriate types)\n    'archive_to_onelake': archive_to_onelake.lower() == \"true\",\n    'max_retries': int(max_retries),\n    'export_timeout': int(export_timeout_seconds),\n    'poll_interval': int(poll_interval_seconds),\n    'retry_backoff_base': int(retry_backoff_base),\n    'download_chunk_size_mb': int(download_chunk_size_mb),\n    'file_size_warning_mb': int(file_size_warning_mb),\n    'connection_timeout': int(connection_timeout_seconds),\n    'download_timeout': int(download_timeout_seconds),\n    'param_loader_max_retries': int(param_loader_retry_attempts),\n    'param_loader_retry_delay': int(param_loader_retry_delay_seconds),\n    'token_refresh_interval': int(token_refresh_interval_minutes)\n}\n\nlogger.info(\"Config dictionary built successfully\")\nlogger.info(f\"  workspace_id: {config['workspace_id'][:8]}... (length={len(config['workspace_id'])})\")\nlogger.info(f\"  report_id: {config['report_id'][:8]}... (length={len(config['report_id'])})\")\nlogger.info(f\"  special_values_source: {config['special_values_source']}\")\n\n\n# ============================================================================\n# INPUT VALIDATOR CLASS\n# ============================================================================\n\nclass InputValidator:\n    \"\"\"Validate and sanitize user inputs to prevent injection attacks\"\"\"\n    \n    @staticmethod\n    def is_valid_guid(value: str) -> bool:\n        \"\"\"Validate GUID format\"\"\"\n        guid_pattern = r'^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$'\n        return bool(re.match(guid_pattern, value.lower())) if value else False\n    \n    @staticmethod\n    def is_valid_sql_identifier(value: str) -> bool:\n        \"\"\"Validate SQL identifier (table/column name) - alphanumeric, underscore, dot only\"\"\"\n        pattern = r'^[a-zA-Z_][a-zA-Z0-9_\\.]*$'\n        return bool(re.match(pattern, value)) and len(value) <= 128\n    \n    @staticmethod\n    def is_valid_source_type(value: str) -> bool:\n        \"\"\"Validate parameter source type\"\"\"\n        return value in [\"semantic_model\", \"lakehouse\", \"json\", \"warehouse\"]\n    \n    @staticmethod\n    def is_valid_format(value: str) -> bool:\n        \"\"\"Validate output format\"\"\"\n        return value.upper() in [\"PDF\", \"XLSX\", \"DOCX\", \"PPTX\", \"PNG\"]\n    \n    @staticmethod\n    def sanitize_sql_string(value: str) -> str:\n        \"\"\"Sanitize string for use in SQL - escape single quotes\"\"\"\n        if value is None:\n            return \"\"\n        return str(value).replace(\"'\", \"''\")\n\n\n# ============================================================================\n# TOKEN MANAGER CLASS\n# ============================================================================\n\nclass TokenManager:\n    \"\"\"Manage Power BI API tokens with automatic refresh for long-running batches\n    \n    Power BI tokens typically expire after 1 hour. This class tracks token age\n    and refreshes proactively to prevent mid-batch authentication failures.\n    \"\"\"\n    \n    def __init__(self, mssparkutils, refresh_interval_minutes: int = 45):\n        \"\"\"Initialize token manager\"\"\"\n        self.mssparkutils = mssparkutils\n        self.refresh_interval = timedelta(minutes=refresh_interval_minutes)\n        self.powerbi_token = None\n        self.token_acquired_at = None\n        self.refresh_tokens()  # Acquire initial tokens\n    \n    def refresh_tokens(self) -> Tuple[str, Dict[str, str]]:\n        \"\"\"Refresh Power BI API tokens\"\"\"\n        try:\n            self.powerbi_token = self.mssparkutils.credentials.getToken(\"pbi\")\n            self.token_acquired_at = datetime.now(timezone.utc)\n            logger.info(\"‚úì Power BI API token refreshed successfully\")\n            return self.powerbi_token, self.get_headers()\n        except Exception as e:\n            logger.error(\"‚ùå Failed to refresh Power BI API token\")\n            raise ValueError(f\"Token refresh failed: {str(e)[:200]}\")\n    \n    def get_headers(self) -> Dict[str, str]:\n        \"\"\"Get current API headers with bearer token\"\"\"\n        return {\"Authorization\": f\"Bearer {self.powerbi_token}\", \"Content-Type\": \"application/json\"}\n    \n    def ensure_valid_token(self) -> Dict[str, str]:\n        \"\"\"Ensure token is valid, refresh if needed\"\"\"\n        if self.token_acquired_at is None:\n            return self.refresh_tokens()[1]  # BUG FIX: Added return statement\n        else:\n            time_since_refresh = datetime.now(timezone.utc) - self.token_acquired_at\n            if time_since_refresh >= self.refresh_interval:\n                logger.info(f\"üîÑ Token is {time_since_refresh.total_seconds()/60:.1f} minutes old, refreshing...\")\n                self.refresh_tokens()\n        return self.get_headers()\n\n\n# ============================================================================\n# PARAMETER LOADER CLASS\n# ============================================================================\n\nclass ParameterLoader:\n    \"\"\"Load special parameter values from multiple sources with security and retry logic\"\"\"\n    \n    def __init__(self, mssparkutils, spark=None, max_retries: int = 3, retry_delay: int = 5):\n        \"\"\"Initialize parameter loader\"\"\"\n        self.mssparkutils = mssparkutils\n        self.spark = spark\n        self.max_retries = max_retries\n        self.retry_delay = retry_delay\n        self.validator = InputValidator()\n    \n    def load(self, source_type: str, **config) -> List[str]:\n        \"\"\"Load parameters with retry logic and deduplication\"\"\"\n        if not self.validator.is_valid_source_type(source_type):\n            raise ValueError(f\"Invalid source type: {source_type}. Must be one of: semantic_model, lakehouse, json, warehouse\")\n        \n        last_exception = None\n        for attempt in range(self.max_retries):\n            try:\n                if source_type == \"semantic_model\":\n                    values = self._load_from_semantic_model(**config)\n                elif source_type == \"lakehouse\":\n                    values = self._load_from_lakehouse(**config)\n                elif source_type == \"json\":\n                    values = self._load_from_json(**config)\n                elif source_type == \"warehouse\":\n                    values = self._load_from_warehouse(**config)\n                \n                # Deduplicate while preserving order\n                seen = set()\n                unique_values = []\n                duplicates = []\n                for v in values:\n                    if v not in seen:\n                        seen.add(v)\n                        unique_values.append(v)\n                    else:\n                        duplicates.append(v)\n                \n                if duplicates:\n                    logger.warning(f\"‚ö† Removed {len(duplicates)} duplicate value(s) from parameter list\")\n                    logger.debug(f\"  Duplicates: {duplicates[:10]}\")\n                \n                return unique_values\n                \n            except Exception as e:\n                last_exception = e\n                if attempt < self.max_retries - 1:\n                    logger.warning(f\"‚ö† Parameter loading attempt {attempt + 1} failed: {str(e)[:200]}\")\n                    logger.info(f\"  ‚Üí Retrying in {self.retry_delay} seconds...\")\n                    time.sleep(self.retry_delay)\n                else:\n                    logger.error(f\"‚ùå All {self.max_retries} parameter loading attempts failed\")\n                    raise Exception(f\"Failed to load parameters from {source_type} after {self.max_retries} attempts: {str(last_exception)[:500]}\")\n    \n    def _load_from_semantic_model(self, workspace_id: str, dataset_id: str, dax_query: str, **kwargs) -> List[str]:\n        \"\"\"Load from Semantic Model (Power BI Dataset) via sempy\"\"\"\n        if not self.validator.is_valid_guid(workspace_id):\n            raise ValueError(f\"Invalid workspace GUID format: {workspace_id}\")\n        if not self.validator.is_valid_guid(dataset_id):\n            raise ValueError(f\"Invalid dataset GUID format: {dataset_id}\")\n        if not dax_query or len(dax_query) > 10000:\n            raise ValueError(\"DAX query must be between 1 and 10000 characters\")\n        \n        logger.info(f\"üìä Querying Semantic Model...\")\n        logger.info(f\"   Workspace: {workspace_id[:8]}...\")\n        logger.info(f\"   Dataset: {dataset_id[:8]}...\")\n        \n        df = fabric.evaluate_dax(dataset=dataset_id, dax_string=dax_query, workspace=workspace_id)\n        if df.empty:\n            raise ValueError(\"Semantic model query returned no results\")\n        \n        column_name = df.columns[0]\n        values = [str(v) for v in df[column_name].tolist() if v is not None and str(v).strip()]\n        logger.info(f\"‚úì Loaded {len(values)} values from Semantic Model\")\n        return values\n    \n    def _load_from_lakehouse(self, table: str, category: str, column: str, filter_clause: str = \"\", **kwargs) -> List[str]:\n        \"\"\"Load from Lakehouse Delta table via Spark SQL\"\"\"\n        if self.spark is None:\n            raise Exception(\"Spark session not available for lakehouse source\")\n        if not self.validator.is_valid_sql_identifier(table):\n            raise ValueError(f\"Invalid table name: {table}\")\n        if not self.validator.is_valid_sql_identifier(column):\n            raise ValueError(f\"Invalid column name: {column}\")\n        \n        safe_category = self.validator.sanitize_sql_string(category)\n        logger.info(f\"üìä Executing Lakehouse query...\")\n        \n        query = f\"SELECT {column} FROM {table} WHERE IsActive = true AND Category = '{safe_category}'\"\n        if filter_clause:\n            dangerous_patterns = [';', '--', '/*', '*/', 'xp_', 'sp_', 'DROP', 'DELETE', 'TRUNCATE', 'ALTER', 'CREATE']\n            if any(pattern.lower() in filter_clause.lower() for pattern in dangerous_patterns):\n                raise ValueError(f\"Filter clause contains potentially dangerous SQL keywords\")\n            query += f\" AND ({filter_clause})\"\n        query += \" ORDER BY SortOrder, ParameterName\"\n        \n        df = self.spark.sql(query)\n        values = [str(row[0]) for row in df.collect() if row[0] is not None]\n        logger.info(f\"‚úì Loaded {len(values)} values from Lakehouse\")\n        return values\n    \n    def _load_from_json(self, json_values: str, **kwargs) -> List[str]:\n        \"\"\"Load from JSON array\"\"\"\n        logger.info(f\"üìä Loading from JSON array...\")\n        if not json_values or json_values.strip() == \"[]\":\n            raise ValueError(\"JSON parameter values cannot be empty\")\n        \n        try:\n            values = json.loads(json_values) if isinstance(json_values, str) else json_values\n        except json.JSONDecodeError as e:\n            raise ValueError(f\"Invalid JSON format: {str(e)}\")\n        \n        if not isinstance(values, list) or len(values) == 0:\n            raise ValueError(\"JSON parameter values must be a non-empty array\")\n        \n        if len(values) > 1000:\n            logger.warning(f\"‚ö† JSON array has {len(values)} values. Consider using Lakehouse or Warehouse for large lists.\")\n        \n        values = [str(v) for v in values if v is not None and str(v).strip()]\n        logger.info(f\"‚úì Loaded {len(values)} values from JSON\")\n        return values\n    \n    def _load_from_warehouse(self, warehouse_name: str, table: str, column: str, category: str = \"\", **kwargs) -> List[str]:\n        \"\"\"Load from Warehouse via SQL endpoint\"\"\"\n        import pyodbc\n        \n        if not warehouse_name or len(warehouse_name) > 128:\n            raise ValueError(\"Invalid warehouse name\")\n        if not self.validator.is_valid_sql_identifier(table):\n            raise ValueError(f\"Invalid table name: {table}\")\n        if not self.validator.is_valid_sql_identifier(column):\n            raise ValueError(f\"Invalid column name: {column}\")\n        \n        logger.info(f\"üìä Querying Warehouse...\")\n        conn = None\n        cursor = None\n        \n        try:\n            token = self.mssparkutils.credentials.getToken(\"sql\")\n            token_bytes = token.encode(\"UTF-16-LE\")\n            token_struct = struct.pack(f'<I{len(token_bytes)}s', len(token_bytes), token_bytes)\n            SQL_COPT_SS_ACCESS_TOKEN = 1256\n            \n            workspace_name = self.mssparkutils.runtime.context.get('workspaceName')\n            if not workspace_name:\n                raise ValueError(\"Could not determine workspace name from context\")\n            \n            conn_str = f\"Driver={{ODBC Driver 18 for SQL Server}};Server={workspace_name}.datawarehouse.fabric.microsoft.com;Database={warehouse_name};Encrypt=yes;TrustServerCertificate=no;\"\n            conn = pyodbc.connect(conn_str, attrs_before={SQL_COPT_SS_ACCESS_TOKEN: token_struct}, timeout=30)\n            cursor = conn.cursor()\n            \n            safe_category = self.validator.sanitize_sql_string(category)\n            query = f\"SELECT {column} FROM {table} WHERE IsActive = 1\"\n            if category:\n                query += f\" AND Category = '{safe_category}'\"\n            query += \" ORDER BY SortOrder\"\n            \n            cursor.execute(query)\n            values = [str(row[0]) for row in cursor.fetchall() if row[0] is not None]\n            logger.info(f\"‚úì Loaded {len(values)} values from Warehouse\")\n            return values\n            \n        except Exception as e:\n            raise Exception(f\"Warehouse query failed: {str(e)[:500]}\")\n        finally:\n            if cursor:\n                try:\n                    cursor.close()\n                except:\n                    pass\n            if conn:\n                try:\n                    conn.close()\n                except:\n                    pass\n\n\n# ============================================================================\n# POWER BI API CLIENT CLASS\n# ============================================================================\n\nclass PowerBIAPIClient:\n    \"\"\"Power BI REST API interactions with retry logic\"\"\"\n    \n    def __init__(self, token_manager: TokenManager, config: Dict):\n        \"\"\"Initialize API client\"\"\"\n        self.token_manager = token_manager\n        self.workspace_id = config['workspace_id']\n        self.report_id = config['report_id']\n        self.output_format = config['output_format']\n        self.connection_timeout = config['connection_timeout']\n        self.poll_interval = config['poll_interval']\n        self.export_timeout = config['export_timeout']\n        self.download_chunk_size_mb = config['download_chunk_size_mb']\n        self.file_size_warning_mb = config['file_size_warning_mb']\n        self.download_timeout = config['download_timeout']\n    \n    def _handle_api_response(self, response: requests.Response, operation: str) -> None:\n        \"\"\"Handle API response with proper error handling and rate limiting\"\"\"\n        if response.status_code == 429:\n            retry_after = int(response.headers.get('Retry-After', '60'))\n            logger.warning(f\"‚ö† API rate limit hit during {operation}\")\n            logger.info(f\"  Waiting {retry_after} seconds before retry...\")\n            time.sleep(retry_after)\n            raise Exception(f\"Rate limited: {operation}. Retry after {retry_after}s\")\n        elif response.status_code >= 400:\n            error_msg = f\"API error during {operation}: HTTP {response.status_code}\"\n            try:\n                error_detail = response.json().get('error', {}).get('message', response.text[:200])\n                error_msg += f\" - {error_detail}\"\n            except:\n                error_msg += f\" - {response.text[:200]}\"\n            raise Exception(error_msg)\n    \n    def initiate_export(self, parameters: Dict[str, Any]) -> str:\n        \"\"\"Initiate paginated report export via Power BI REST API\"\"\"\n        export_url = f\"https://api.powerbi.com/v1.0/myorg/groups/{self.workspace_id}/reports/{self.report_id}/ExportTo\"\n        headers = self.token_manager.get_headers()\n        body = {\n            \"format\": self.output_format,\n            \"paginatedReportConfiguration\": {\n                \"parameterValues\": [{\"name\": k, \"value\": str(v)} for k, v in parameters.items()]\n            }\n        }\n        response = requests.post(export_url, headers=headers, json=body, timeout=self.connection_timeout)\n        self._handle_api_response(response, \"initiate export\")\n        return response.json()['id']\n    \n    def poll_status(self, export_id: str) -> bool:\n        \"\"\"Poll export status until completion or timeout\"\"\"\n        status_url = f\"https://api.powerbi.com/v1.0/myorg/groups/{self.workspace_id}/reports/{self.report_id}/exports/{export_id}\"\n        headers = self.token_manager.get_headers()\n        start_time = time.time()\n        poll_count = 0\n        \n        while time.time() - start_time < self.export_timeout:\n            poll_count += 1\n            try:\n                response = requests.get(status_url, headers=headers, timeout=self.connection_timeout)\n                self._handle_api_response(response, \"poll export status\")\n                status_data = response.json()\n                status = status_data.get('status')\n                \n                if status == 'Succeeded':\n                    logger.debug(f\"Export succeeded after {poll_count} polls ({time.time() - start_time:.1f}s)\")\n                    return True\n                elif status == 'Failed':\n                    error = status_data.get('error', {}).get('message', 'Unknown error')\n                    raise Exception(f\"Export failed: {error}\")\n                elif status in ['Running', 'NotStarted']:\n                    time.sleep(self.poll_interval)\n                else:\n                    logger.warning(f\"‚ö† Unknown export status: {status}\")\n                    time.sleep(self.poll_interval)\n            except Exception as e:\n                if \"Rate limited\" in str(e):\n                    raise\n                logger.warning(f\"‚ö† Error during status poll {poll_count}: {str(e)[:200]}\")\n                time.sleep(self.poll_interval)\n        \n        raise TimeoutError(f\"Export timeout after {self.export_timeout} seconds ({poll_count} polls)\")\n    \n    def download_file(self, export_id: str) -> bytes:\n        \"\"\"Download exported report file\"\"\"\n        file_url = f\"https://api.powerbi.com/v1.0/myorg/groups/{self.workspace_id}/reports/{self.report_id}/exports/{export_id}/file\"\n        headers = self.token_manager.get_headers()\n        response = requests.get(file_url, headers=headers, stream=True, timeout=self.download_timeout)\n        self._handle_api_response(response, \"download report file\")\n        \n        content = b''\n        chunk_size_bytes = self.download_chunk_size_mb * 1024 * 1024\n        file_size_warning_bytes = self.file_size_warning_mb * 1024 * 1024\n        \n        for chunk in response.iter_content(chunk_size=chunk_size_bytes):\n            if chunk:\n                content += chunk\n                if len(content) > file_size_warning_bytes:\n                    logger.warning(f\"‚ö† Report file exceeds {self.file_size_warning_mb}MB, may cause memory issues\")\n        \n        return content\n\n\n# ============================================================================\n# ONELAKE STORAGE CLASS\n# ============================================================================\n\nclass OneLakeStorage:\n    \"\"\"File storage operations with sanitization and date-based hierarchy\"\"\"\n    \n    def __init__(self, mssparkutils, config: Dict):\n        \"\"\"Initialize storage handler\"\"\"\n        self.mssparkutils = mssparkutils\n        self.output_format = config['output_format']\n        self.special_param_name = config['special_param_name']\n        self.file_size_warning_mb = config['file_size_warning_mb']\n    \n    def _sanitize_filename(self, text: str, max_length: int = 200) -> str:\n        \"\"\"Sanitize text for use in filename with Unicode support\"\"\"\n        if not text:\n            return \"unnamed\"\n        \n        text = str(text)\n        text = unicodedata.normalize('NFKD', text)\n        text = text.encode('ascii', 'ignore').decode('ascii')\n        text = re.sub(r'[^\\w\\s-]', '', text)\n        text = re.sub(r'\\s+', ' ', text)\n        text = text.replace(' ', '_')\n        text = text.strip('_-')\n        \n        if not text:\n            text = \"unnamed\"\n        if len(text) > max_length:\n            text = text[:max_length]\n        \n        return text\n    \n    def _generate_file_name(self, special_value: str, timestamp: datetime = None) -> str:\n        \"\"\"Generate descriptive filename with timezone-aware timestamp\n        \n        Format: Report_{ParamName}_{SanitizedValue}_{Timestamp}.{ext}\n        Example: Report_Producer_AcmeCorp_20250130_143022.pdf\n        \"\"\"\n        if timestamp is None:\n            timestamp = datetime.now(timezone.utc)\n        elif timestamp.tzinfo is None:\n            timestamp = timestamp.replace(tzinfo=timezone.utc)\n        \n        sanitized_value = self._sanitize_filename(special_value, max_length=100)\n        sanitized_param = self._sanitize_filename(self.special_param_name, max_length=50)\n        timestamp_str = timestamp.strftime('%Y%m%d_%H%M%S')\n        extension = self.output_format.lower()\n        \n        filename = f\"Report_{sanitized_param}_{sanitized_value}_{timestamp_str}.{extension}\"\n        \n        # Final safety check\n        if len(filename) > 255:\n            sanitized_value = self._sanitize_filename(special_value, max_length=50)\n            filename = f\"Report_{sanitized_param}_{sanitized_value}_{timestamp_str}.{extension}\"\n        \n        return filename\n    \n    def save(self, file_content: bytes, special_value: str) -> str:\n        \"\"\"Save binary file to OneLake for archival with date-based organization\n        \n        This function properly handles binary content (PDF, XLSX, etc.) by:\n        1. Writing to a temporary file first\n        2. Copying to OneLake using mssparkutils.fs.cp()\n        3. Cleaning up the temporary file\n        \n        Note: Requires a Lakehouse to be attached to the notebook as a data item.\n        \"\"\"\n        timestamp = datetime.now(timezone.utc)\n        date_path = timestamp.strftime('%Y/%m/%d')\n        filename = self._generate_file_name(special_value, timestamp)\n        onelake_path = f\"Files/reports/archive/{date_path}/{filename}\"\n        \n        file_size_mb = len(file_content) / (1024 * 1024)\n        if file_size_mb > self.file_size_warning_mb:\n            logger.warning(f\"‚ö† File size is {file_size_mb:.1f}MB, which is very large\")\n        \n        temp_file = None\n        temp_path = None\n        try:\n            # Create a temporary file to write binary content\n            with tempfile.NamedTemporaryFile(delete=False, suffix=f\".{self.output_format.lower()}\") as temp_file:\n                temp_file.write(file_content)\n                temp_path = temp_file.name\n            \n            # Copy from temp file to OneLake using file:// protocol\n            temp_url = f\"file://{temp_path}\"\n            \n            # Delete existing file if it exists (since cp doesn't support overwrite parameter)\n            try:\n                self.mssparkutils.fs.rm(onelake_path)\n            except Exception:\n                pass  # File doesn't exist, that's fine\n            \n            # Use mssparkutils.fs.cp to copy the file to OneLake\n            self.mssparkutils.fs.cp(temp_url, onelake_path)\n            \n            logger.debug(f\"  Saved {file_size_mb:.2f}MB to OneLake\")\n            \n        except Exception as e:\n            error_msg = str(e)\n            # Don't include binary content in error message\n            if len(error_msg) > 200:\n                error_msg = error_msg[:200]\n            raise Exception(f\"Failed to write to OneLake: {error_msg}\")\n        finally:\n            # Clean up temporary file\n            if temp_path and os.path.exists(temp_path):\n                try:\n                    os.unlink(temp_path)\n                except Exception:\n                    pass  # Ignore cleanup errors\n        \n        return onelake_path\n\n\n# ============================================================================\n# PAGINATED REPORT EXECUTOR CLASS (Main Orchestrator)\n# ============================================================================\n\nclass PaginatedReportExecutor:\n    \"\"\"Main orchestrator for paginated report batch execution using composition\"\"\"\n    \n    def __init__(self, config: Dict, mssparkutils, spark=None):\n        \"\"\"Initialize executor and all components\"\"\"\n        self.config = config\n        self.mssparkutils = mssparkutils\n        self.spark = spark\n        \n        # Initialize logger\n        logger.info(\"\\n\" + \"=\"*60)\n        logger.info(\"INITIALIZING PAGINATED REPORT EXECUTOR\")\n        logger.info(\"=\"*60 + \"\\n\")\n        \n        # Compose helper objects\n        self.validator = InputValidator()\n        self.token_manager = TokenManager(\n            mssparkutils, \n            refresh_interval_minutes=config['token_refresh_interval']\n        )\n        self.param_loader = ParameterLoader(\n            mssparkutils, \n            spark,\n            max_retries=config['param_loader_max_retries'],\n            retry_delay=config['param_loader_retry_delay']\n        )\n        self.api_client = PowerBIAPIClient(self.token_manager, config)\n        \n        # Storage is optional (only if archiving enabled)\n        if config['archive_to_onelake']:\n            self.storage = OneLakeStorage(mssparkutils, config)\n        else:\n            self.storage = None\n        \n        # Validate all parameters\n        self._validate_all_parameters()\n        \n        # Log configuration\n        self._log_configuration()\n        \n        logger.info(\"‚úì Executor initialized successfully\\n\")\n    \n    def _validate_all_parameters(self):\n        \"\"\"Validate all configuration parameters\"\"\"\n        if not self.validator.is_valid_guid(self.config['workspace_id']):\n            raise ValueError(f\"Invalid workspace_id format. Must be a valid GUID. Received: '{self.config['workspace_id']}'\")\n        if not self.validator.is_valid_guid(self.config['report_id']):\n            raise ValueError(f\"Invalid report_id format. Must be a valid GUID. Received: '{self.config['report_id']}'\")\n        if not self.validator.is_valid_format(self.config['output_format']):\n            raise ValueError(f\"Invalid output_format: {self.config['output_format']}. Must be one of: PDF, XLSX, DOCX, PPTX, PNG\")\n        if not self.validator.is_valid_source_type(self.config['special_values_source']):\n            raise ValueError(f\"Invalid special_values_source: {self.config['special_values_source']}. Must be one of: semantic_model, lakehouse, json, warehouse\")\n        \n        # Validate source-specific parameters based on selected source\n        logger.info(f\"üìã Validating configuration for source type: {self.config['special_values_source']}\")\n        \n        source_type = self.config['special_values_source']\n        if source_type == \"semantic_model\":\n            source_config = self.config['semantic_model']\n            if not self.validator.is_valid_guid(source_config['workspace_id']):\n                raise ValueError(f\"Invalid semantic_model workspace_id format. Required for semantic_model source.\")\n            if not self.validator.is_valid_guid(source_config['dataset_id']):\n                raise ValueError(f\"Invalid semantic_model dataset_id format. Required for semantic_model source.\")\n            if not source_config['dax_query']:\n                raise ValueError(\"semantic_model_dax_query is required for semantic_model source\")\n            logger.info(f\"‚úì Semantic Model configuration validated\")\n        \n        elif source_type == \"lakehouse\":\n            source_config = self.config['lakehouse']\n            if not source_config['table']:\n                raise ValueError(\"lakehouse_table is required for lakehouse source\")\n            if not source_config['category']:\n                raise ValueError(\"lakehouse_category is required for lakehouse source\")\n            if not source_config['column']:\n                raise ValueError(\"lakehouse_column is required for lakehouse source\")\n            logger.info(f\"‚úì Lakehouse configuration validated\")\n        \n        elif source_type == \"json\":\n            source_config = self.config['json']\n            if not source_config['json_values'] or source_config['json_values'].strip() == \"[]\":\n                raise ValueError(\"special_parameter_values is required for json source and cannot be empty\")\n            logger.info(f\"‚úì JSON configuration validated\")\n        \n        elif source_type == \"warehouse\":\n            source_config = self.config['warehouse']\n            if not source_config['warehouse_name']:\n                raise ValueError(\"warehouse_name is required for warehouse source\")\n            if not source_config['table']:\n                raise ValueError(\"warehouse_table is required for warehouse source\")\n            if not source_config['column']:\n                raise ValueError(\"warehouse_column is required for warehouse source\")\n            logger.info(f\"‚úì Warehouse configuration validated\")\n        \n        # Validate static parameters\n        if not isinstance(self.config['static_params'], dict):\n            raise ValueError(\"static_params must be a dictionary\")\n        logger.info(f\"‚úì Static parameters validated: {len(self.config['static_params'])} parameter(s)\")\n        \n        logger.info(\"‚úì All configuration validated successfully\")\n    \n    def _log_configuration(self):\n        \"\"\"Log startup configuration\"\"\"\n        logger.info(\"Configuration:\")\n        logger.info(f\"  Report ID: {self.config['report_id'][:8] if self.config['report_id'] else 'Not set'}...\")\n        logger.info(f\"  Special parameter: {self.config['special_param_name']}\")\n        logger.info(f\"  Source: {self.config['special_values_source']}\")\n        logger.info(f\"  Output format: {self.config['output_format']}\")\n        logger.info(f\"  Archive to OneLake: {self.config['archive_to_onelake']}\")\n        logger.info(f\"  Export timeout: {self.config['export_timeout']}s\")\n        logger.info(f\"  Max retries: {self.config['max_retries']}\")\n        logger.info(f\"  Token refresh interval: {self.config['token_refresh_interval']} minutes\")\n    \n    def _load_special_values(self) -> List[str]:\n        \"\"\"Load special parameter values from configured source\"\"\"\n        logger.info(\"\\n\" + \"=\"*60)\n        logger.info(\"LOADING SPECIAL PARAMETER VALUES\")\n        logger.info(\"=\"*60 + \"\\n\")\n        \n        source_type = self.config['special_values_source']\n        source_config = self.config[source_type]\n        \n        try:\n            special_values = self.param_loader.load(source_type, **source_config)\n        except Exception as e:\n            logger.error(f\"‚ùå Failed to load parameters: {str(e)[:500]}\")\n            logger.error(f\"   Source: {source_type}\")\n            logger.error(f\"   Check your configuration and ensure:\")\n            logger.error(f\"   - Data source exists and is accessible\")\n            logger.error(f\"   - Permissions are granted\")\n            logger.error(f\"   - Parameters are correctly formatted\")\n            raise\n        \n        logger.info(f\"\\n{'='*60}\")\n        logger.info(f\"Parameter '{self.config['special_param_name']}' loaded: {len(special_values)} unique values\")\n        if len(special_values) <= 10:\n            logger.info(f\"Values: {special_values}\")\n        else:\n            logger.info(f\"First 10 values: {special_values[:10]}\")\n            logger.info(f\"... and {len(special_values) - 10} more\")\n        logger.info(f\"{'='*60}\\n\")\n        \n        # Validation\n        if not special_values or len(special_values) == 0:\n            raise ValueError(\n                f\"‚ùå No parameter values loaded from {source_type}! \"\n                f\"Check your configuration:\\n\"\n                f\"  - Ensure the data source has data\\n\"\n                f\"  - Verify category/filter settings\\n\"\n                f\"  - Check permissions\"\n            )\n        \n        # Warnings for large batches\n        if len(special_values) > 100:\n            estimated_minutes = len(special_values) * 2\n            logger.warning(f\"‚ö† Processing {len(special_values)} values may take a long time\")\n            logger.warning(f\"  Estimated time: {estimated_minutes} minutes (assuming 2 min per report)\")\n            logger.warning(f\"  Token will auto-refresh every {self.config['token_refresh_interval']} minutes\")\n        \n        if len(special_values) > 500:\n            logger.warning(f\"‚ö† Very large batch detected! This may take hours to complete.\")\n            logger.warning(f\"  Recommendation: Use multiple pipelines to process in parallel\")\n        \n        logger.info(\"‚úì Special parameter values loaded, deduplicated, and validated\")\n        \n        return special_values\n    \n    def _execute_single_report(self, params: Dict, special_value: str) -> Dict:\n        \"\"\"Execute report for a single parameter value with retry logic\"\"\"\n        start_time = datetime.now(timezone.utc)\n        \n        for attempt in range(self.config['max_retries']):\n            try:\n                # Ensure token is valid (refreshes if needed)\n                self.token_manager.ensure_valid_token()\n                \n                # Step 1: Initiate export\n                logger.info(f\"  Step 1/4: Initiating report export...\")\n                export_id = self.api_client.initiate_export(params)\n                logger.info(f\"    ‚úì Export initiated. Export ID: {export_id[:8]}...\")\n                \n                # Step 2: Poll for completion\n                logger.info(f\"  Step 2/4: Waiting for export to complete...\")\n                self.api_client.poll_status(export_id)\n                logger.info(f\"    ‚úì Export completed successfully\")\n                \n                # Step 3: Download file\n                logger.info(f\"  Step 3/4: Downloading report file...\")\n                file_content = self.api_client.download_file(export_id)\n                file_size_mb = len(file_content) / (1024 * 1024)\n                logger.info(f\"    ‚úì Downloaded {file_size_mb:.2f} MB\")\n                \n                # Step 4: Save to OneLake (if enabled)\n                onelake_path = None\n                if self.storage:\n                    logger.info(f\"  Step 4/4: Saving to OneLake archive...\")\n                    onelake_path = self.storage.save(file_content, special_value)\n                    logger.info(f\"    ‚úì Saved to: {onelake_path}\")\n                \n                # Success!\n                end_time = datetime.now(timezone.utc)\n                duration = (end_time - start_time).total_seconds()\n                \n                return {\n                    'special_value': special_value,\n                    'status': 'SUCCESS',\n                    'onelake_path': onelake_path,\n                    'file_size_mb': round(file_size_mb, 2),\n                    'duration_seconds': round(duration, 2),\n                    'timestamp': end_time.isoformat(),\n                    'attempts': attempt + 1,\n                    'error': None\n                }\n            \n            except Exception as e:\n                error_msg = str(e)[:500]\n                \n                if attempt < self.config['max_retries'] - 1:\n                    wait_time = self.config['retry_backoff_base'] * (2 ** attempt)\n                    logger.warning(f\"  ‚úó Attempt {attempt + 1}/{self.config['max_retries']} failed: {error_msg}\")\n                    logger.info(f\"  ‚Üí Retrying in {wait_time} seconds...\")\n                    time.sleep(wait_time)\n                else:\n                    # All retries exhausted\n                    end_time = datetime.now(timezone.utc)\n                    duration = (end_time - start_time).total_seconds()\n                    logger.error(f\"  ‚úó All {self.config['max_retries']} attempts failed for '{special_value}'\")\n                    logger.error(f\"     Final error: {error_msg}\")\n                    \n                    return {\n                        'special_value': special_value,\n                        'status': 'FAILED',\n                        'onelake_path': None,\n                        'file_size_mb': 0,\n                        'duration_seconds': round(duration, 2),\n                        'timestamp': end_time.isoformat(),\n                        'attempts': self.config['max_retries'],\n                        'error': error_msg\n                    }\n        \n        # Should never reach here\n        return {\n            'special_value': special_value,\n            'status': 'FAILED',\n            'error': 'Unknown error - retry loop completed unexpectedly',\n            'attempts': self.config['max_retries']\n        }\n    \n    def _generate_summary(self, results: List[Dict], total_duration: float) -> Dict:\n        \"\"\"Generate execution summary and pipeline result JSON\"\"\"\n        success_count = len([r for r in results if r['status'] == 'SUCCESS'])\n        fail_count = len(results) - success_count\n        total_size_mb = sum(r.get('file_size_mb', 0) for r in results)\n        avg_duration = sum(r.get('duration_seconds', 0) for r in results) / len(results) if results else 0\n        total_attempts = sum(r.get('attempts', 1) for r in results)\n        \n        logger.info(f\"\\n{'='*60}\")\n        logger.info(\"EXECUTION SUMMARY\")\n        logger.info(f\"{'='*60}\")\n        logger.info(f\"Total reports processed: {len(results)}\")\n        logger.info(f\"‚úì Successful: {success_count}\")\n        logger.info(f\"‚úó Failed: {fail_count}\")\n        logger.info(f\"Total size: {total_size_mb:.2f} MB\")\n        logger.info(f\"Average duration per report: {avg_duration:.1f} seconds\")\n        logger.info(f\"Success rate: {(success_count/len(results)*100):.1f}%\")\n        logger.info(f\"Total retry attempts: {total_attempts} (avg {total_attempts/len(results):.1f} per report)\")\n        \n        # Print successful files\n        if success_count > 0:\n            logger.info(f\"\\n{'='*60}\")\n            logger.info(f\"GENERATED FILES (Saved to OneLake): {success_count} files\")\n            logger.info(f\"{'='*60}\")\n            # Show first 20, then summarize\n            for idx, r in enumerate([r for r in results if r['status'] == 'SUCCESS'][:20], 1):\n                logger.info(f\"  {idx}. {r['onelake_path']} ({r['file_size_mb']} MB)\")\n            if success_count > 20:\n                logger.info(f\"  ... and {success_count - 20} more files\")\n        \n        # Print failures\n        if fail_count > 0:\n            logger.info(f\"\\n{'='*60}\")\n            logger.error(f\"FAILURES: {fail_count} reports failed\")\n            logger.info(f\"{'='*60}\")\n            for idx, r in enumerate([r for r in results if r['status'] == 'FAILED'], 1):\n                # Truncate error message for readability\n                error_msg = r.get('error', 'Unknown error')\n                if len(error_msg) > 200:\n                    error_msg = error_msg[:200] + \"...\"\n                logger.error(f\"  {idx}. '{r['special_value']}': {error_msg}\")\n        \n        completion_time = datetime.now(timezone.utc)\n        logger.info(f\"\\n{'='*60}\")\n        logger.info(f\"Execution completed at: {completion_time.strftime('%Y-%m-%d %H:%M:%S UTC')}\")\n        logger.info(f\"{'='*60}\\n\")\n        \n        # Prepare file list for pipeline consumption\n        successful_files = [\n            r['onelake_path'] \n            for r in results \n            if r['status'] == 'SUCCESS' and r.get('onelake_path')\n        ]\n        \n        # Prepare result for pipeline (must match pipeline's expected JSON structure)\n        pipeline_result = {\n            'files': successful_files,\n            'status': 'success' if fail_count == 0 else 'partial_success' if success_count > 0 else 'failed',\n            'total': len(results),\n            'success_count': success_count,\n            'fail_count': fail_count,\n            'total_size_mb': round(total_size_mb, 2),\n            'avg_duration_seconds': round(avg_duration, 2),\n            'total_duration_seconds': round(total_duration, 2),\n            'errors': [\n                {'value': r['special_value'], 'error': r['error'][:200] if r.get('error') else 'Unknown'} \n                for r in results if r['status'] == 'FAILED'\n            ][:50],  # Limit to first 50 errors to avoid huge JSON\n            'timestamp': completion_time.isoformat(),\n            'parameter_name': self.config['special_param_name'],\n            'source_type': self.config['special_values_source']\n        }\n        \n        logger.info(f\"üì§ Returning result to pipeline:\")\n        logger.info(f\"   Status: {pipeline_result['status']}\")\n        logger.info(f\"   Files: {len(successful_files)} paths\")\n        logger.info(f\"   Success: {success_count}/{len(results)}\")\n        \n        return pipeline_result\n    \n    def execute_batch(self) -> Dict:\n        \"\"\"Main execution method - PUBLIC API\"\"\"\n        logger.info(\"\\n\" + \"=\"*60)\n        logger.info(\"STARTING REPORT BATCH EXECUTION\")\n        logger.info(\"=\"*60 + \"\\n\")\n        sys.stdout.flush()\n        \n        execution_start_time = datetime.now(timezone.utc)\n        \n        print(f\"\\n‚è± Execution started at: {execution_start_time}\")\n        sys.stdout.flush()\n        \n        # Load special parameter values\n        special_values = self._load_special_values()\n        \n        print(f\"üìä Total items to process: {len(special_values)}\")\n        print(f\"üîÑ Starting loop...\")\n        sys.stdout.flush()\n        \n        # Execute batch loop\n        results = []\n        for idx, special_value in enumerate(special_values, 1):\n            # IMMEDIATE FEEDBACK - Print to stdout AND logger\n            print(f\"\\n{'='*60}\")\n            print(f\"‚ñ∂ PROCESSING {idx}/{len(special_values)}: {self.config['special_param_name']} = '{special_value}'\")\n            print(f\"{'='*60}\")\n            sys.stdout.flush()\n            \n            logger.info(f\"\\n{'='*60}\")\n            logger.info(f\"Processing {idx}/{len(special_values)}: {self.config['special_param_name']} = '{special_value}'\")\n            logger.info(f\"{'='*60}\\n\")\n            sys.stdout.flush()\n            \n            # Merge static parameters with current special value\n            print(f\"  ‚öô Merging parameters...\")\n            sys.stdout.flush()\n            \n            all_params = self.config['static_params'].copy()\n            all_params[self.config['special_param_name']] = special_value\n            \n            print(f\"  ‚úì Parameters merged\")\n            logger.info(f\"  Parameters:\")\n            for key, value in all_params.items():\n                value_str = str(value)\n                if len(value_str) > 100:\n                    value_str = value_str[:100] + \"...\"\n                logger.info(f\"    {key}: {value_str}\")\n            logger.info(\"\")\n            sys.stdout.flush()\n            \n            # Execute report with retry logic\n            print(f\"  üöÄ Calling execute_report_with_retry...\")\n            sys.stdout.flush()\n            \n            result = self._execute_single_report(all_params, special_value)\n            \n            print(f\"  ‚úì execute_report_with_retry returned\")\n            sys.stdout.flush()\n            \n            results.append(result)\n            \n            # Print result summary with immediate flush\n            if result['status'] == 'SUCCESS':\n                print(f\"  ‚úÖ SUCCESS ({result['duration_seconds']}s, {result['attempts']} attempt(s))\")\n                sys.stdout.flush()\n                logger.info(f\"\\n  ‚úÖ SUCCESS ({result['duration_seconds']}s, {result['attempts']} attempt(s))\")\n                logger.info(f\"     OneLake: {result['onelake_path']}\")\n                logger.info(f\"     Size: {result['file_size_mb']} MB\")\n            else:\n                print(f\"  ‚ùå FAILED ({result['duration_seconds']}s, {result['attempts']} attempt(s))\")\n                print(f\"     Error: {result['error']}\")\n                sys.stdout.flush()\n                logger.error(f\"\\n  ‚ùå FAILED ({result['duration_seconds']}s, {result['attempts']} attempt(s))\")\n                logger.error(f\"     Error: {result['error']}\")\n            \n            sys.stdout.flush()\n            \n            # Progress update for large batches\n            if len(special_values) > 20 and idx % 10 == 0:\n                success_so_far = len([r for r in results if r['status'] == 'SUCCESS'])\n                pct_complete = (idx / len(special_values)) * 100\n                print(f\"\\n  üìä Progress: {idx}/{len(special_values)} ({pct_complete:.1f}%) - {success_so_far} successful\")\n                sys.stdout.flush()\n                logger.info(f\"\\n  üìä Progress: {idx}/{len(special_values)} ({pct_complete:.1f}%) - {success_so_far} successful\")\n        \n        print(\"\\n\" + \"=\" * 60)\n        print(\"LOOP COMPLETED\")\n        print(\"=\" * 60)\n        sys.stdout.flush()\n        \n        execution_end_time = datetime.now(timezone.utc)\n        total_duration = (execution_end_time - execution_start_time).total_seconds()\n        \n        logger.info(f\"\\n{'='*60}\")\n        logger.info(\"EXECUTION COMPLETE\")\n        logger.info(f\"{'='*60}\")\n        logger.info(f\"Total time: {total_duration:.1f} seconds ({total_duration/60:.1f} minutes)\")\n        logger.info(f\"Average per report: {total_duration/len(results):.1f} seconds\")\n        sys.stdout.flush()\n        \n        print(f\"\\n‚úÖ BATCH EXECUTION COMPLETED SUCCESSFULLY\")\n        print(f\"   Processed: {len(results)} reports\")\n        print(f\"   Duration: {total_duration:.1f} seconds\")\n        sys.stdout.flush()\n        \n        # Generate and return summary\n        return self._generate_summary(results, total_duration)\n\n\n# ============================================================================\n# MAIN EXECUTION\n# ============================================================================\n\nif __name__ == \"__main__\":\n    logger.info(f\"\\n{'='*60}\")\n    logger.info(\"PAGINATED REPORT BATCH EXECUTOR v1.0\")\n    logger.info(f\"Started at: {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')}\")\n    logger.info(f\"{'='*60}\\n\")\n    \n    # Create executor with all configuration\n    executor = PaginatedReportExecutor(\n        config=config,\n        mssparkutils=mssparkutils,\n        spark=spark\n    )\n    \n    # Execute batch\n    result = executor.execute_batch()\n    \n    # Exit for pipeline integration\n    logger.info(\"\\nüì§ Exiting notebook with result for pipeline...\")\n    mssparkutils.notebook.exit(json.dumps(result))\n    \n    logger.info(\"‚úì Notebook completed successfully\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}